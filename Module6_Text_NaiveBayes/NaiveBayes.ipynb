{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining for Business Analytics\n",
    "\n",
    "## Dealing with text and Naive Bayes\n",
    "\n",
    "Spring 2019 - Prof. George Valkanas\n",
    "\n",
    "Material based on content courtesy of Prof. Foster Provost and Prof. Panos Ipeirotis\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following good practices, let's load the packages we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "# np.random.seed(36)\n",
    "\n",
    "# We will want to keep track of some different roc curves, lets do that here\n",
    "tprs = []\n",
    "fprs = []\n",
    "roc_labels = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "For this class, we have a new data set in `data/spam_ham.csv`. Let's take a look at what it contains.\n",
    "\n",
    "Let's remember how to use the terminal (UNIX) commands for quick queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 data/spam_ham.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have two features: some text (looks like an email), and a label for spam or ham. What is the distribution of the target variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cut -f2 -d',' data/spam_ham.csv | sort | uniq -c | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't look like that did what we wanted. Can you see why?\n",
    "\n",
    "The data in this file is **text data**. The text data in the first column can have commas. The command line will have some issues reading this data since it will try to split on all instances of the delimeter. Ideally, we would like to have a way of **encapsulating** the first column. Note that we actually have something like this in the data. The first column is wrapped in single quotes. Python (and pandas) have more explicit ways of dealing with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/spam_ham.csv\", quotechar=\"'\", escapechar=\"\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we specify that fields that need to be encapsulated are done so with single quotes (`quotechar`). But, what if the text in this field uses single quotes? For example, apostrophes in words like \"can't\" would break the encapsulation. To overcome this, we **escape** single quotes that are actually just text. Here, we specify the escape character as a backslash (`escapechar`). So now, for example, \"can't\" would be written as \"can\\'t\".\n",
    "\n",
    "(Normally, this would involve slightly painful process of going through, finding the extra quotes, and escaping them.  A common way to find them would be to try to load the data, see where it breaks, fix the problem, and then iterate.  If there are just a few issues to fix, this can be effective.)\n",
    "\n",
    "Let's take another look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the target is whether or not a record should be considered as spam. This is recorded as the string 'spam' or 'ham'. To make it a little easier for our classifier, let's recode it as `0` or `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['spam'] = pd.Series(data['spam'] == 'spam', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to do some modeling, we should split our data into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text']  # These are the features\n",
    "Y = data['spam']  # These are the labels\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text as features\n",
    "How can we turn the large amount of text for each record into useful features?\n",
    "\n",
    "\n",
    "#### Binary representation\n",
    "One way is to create a matrix that uses each word as a feature and keeps track of whether or not a word appears in a document/record. You can do this in sklearn with a `CountVectorizer()` and setting `binary` to `true`. The process is very similar to how you fit a model: you will fit a `CounterVectorizer()`. This will figure out what words exist in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "binary_vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the vocabulary the `CountVectorizer()` learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list = list(zip( binary_vectorizer.vocabulary_.keys(), binary_vectorizer.vocabulary_.values()) )\n",
    "\n",
    "vocabulary_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what words are in the data, we can transform our blobs of text into a clean matrix. Simply `.transform()` the raw data using our fitted `CountVectorizer()`. You will do this for the training and test data. What do you think happens if there are new words in the test data that were not seen in the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_binary = binary_vectorizer.transform(X_train)\n",
    "X_test_binary = binary_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at our new `X_test_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse matrix? Where is our data?\n",
    "\n",
    "If you look at the output above, you will see that it is being stored in a *sparse* matrix (as opposed to the typical dense matrix) that is ~2k rows long and ~70k columns. The rows here are records in the original data and the columns are words. Given the shape, this means there are ~140m cells that should have values. However, from the above, we can see that only ~220k cells (~0.15%) of the cells have values! Why is this?\n",
    "\n",
    "To save space, sklearn uses a sparse matrix. This means that only values that are not zero are stored! This saves a ton of space! This also means that visualizing the data is a little trickier. Let's look at a very small chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_binary[0:20, 0:20].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying a model\n",
    "Now that we have a ton of features (since we have a ton of words!) let's try using a logistic regression model to predict spam/ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_binary, Y_train)\n",
    "\n",
    "print (\"Area under the ROC curve on test data = %.3f\" % metrics.roc_auc_score(Y_test, model.predict(X_test_binary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this any good? What do we care about in this case? Let's take a look at our ROC measure in more detail by looking at the actual ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test, model.predict_proba(X_test_binary)[:,1])\n",
    "\n",
    "#tprs.append(tpr)\n",
    "#fprs.append(fpr)\n",
    "#roc_labels.append(\"Default Binary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = plt.subplot()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"fpr\")\n",
    "plt.ylabel(\"tpr\")\n",
    "plt.title(\"ROC Curve\")\n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "**Take some time and review the curve. What do you think? Is it good? Is it bad?** \n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test, model.predict_proba(X_test_binary)[:,1])\n",
    "\n",
    "tprs.append(tpr)\n",
    "fprs.append(fpr)\n",
    "roc_labels.append(\"Default Binary\")\n",
    "\n",
    "ax = plt.subplot()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"fpr\")\n",
    "plt.ylabel(\"tpr\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlim([-0.01, .07])\n",
    "plt.ylim([.98, 1.001])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counts instead of binary\n",
    "Instead of using a 0 or 1 to represent the occurence of a word, we can use the actual counts. We do this the same way as before, but now we leave `binary` set to `false` (the default value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a counter\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(X_train)\n",
    "\n",
    "# Transform to counter\n",
    "X_train_counts = count_vectorizer.transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_counts, Y_train)\n",
    "\n",
    "print (\"Area under the ROC curve on test data = %.3f\" % metrics.roc_auc_score(Y_test, model.predict(X_test_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test, model.predict_proba(X_test_counts)[:,1])\n",
    "\n",
    "tprs.append(tpr)\n",
    "fprs.append(fpr)\n",
    "roc_labels.append(\"Default Counts\")\n",
    "\n",
    "ax = plt.subplot()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"fpr\")\n",
    "plt.ylabel(\"tpr\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlim([0, .07])\n",
    "plt.ylim([.98, 1.001])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-idf\n",
    "Another popular technique when dealing with text is to use the term frequency - inverse document frequency (tf-idf) measure instead of just counts as the feature values (see text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a counter\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "# Transform to a counter\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "print (\"Area under the ROC curve on test data = %.3f\" % metrics.roc_auc_score(Y_test, model.predict(X_test_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can look at the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test, model.predict_proba(X_test_tfidf)[:,1])\n",
    "\n",
    "tprs.append(tpr)\n",
    "fprs.append(fpr)\n",
    "roc_labels.append(\"Default Tfidf\")\n",
    "\n",
    "ax = plt.subplot()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"fpr\")\n",
    "plt.ylabel(\"tpr\")\n",
    "plt.title(\"ROC Curve\")\n",
    "\n",
    "plt.xlim([-0.01, .07])\n",
    "plt.ylim([.98, 1.001])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CountVectorizer()` and `TfidfVectorizer()` functions have many options. You can restrict the words you would like in the vocabulary. You can add n-grams. You can use stop word lists. Which options you should use generally depend on the type of data you are dealing with. We can discuss and try some of them now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a few different feature sets and models, let's look at all of our ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fpr, tpr, roc_label in zip(fprs, tprs, roc_labels):\n",
    "    plt.plot(fpr, tpr, label=roc_label)\n",
    "\n",
    "plt.xlabel(\"fpr\")\n",
    "plt.ylabel(\"tpr\")\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim([0, .07])\n",
    "plt.ylim([.98, 1.001])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling with another technique: Naive Bayes\n",
    "\n",
    "So far we have been exposed to tree classifiers and logistic regression in class. We have also seen SVM model in the homework. Now, it's time for another popular modeling technique of supervised learning(especially in text classification): the Naive Bayes (NB) classifier. In particular, we are using a Bernoulli Naive Bayes (BNB) for our binary classification. (Bernoulli NB is the model described in the book; there are other versions of NB, see below.)\n",
    "\n",
    "As described in your text, the Naive Bayes model is a **probabilistic approach which assumes conditional independence between features** (in this case, each word is a feature, the conditioning is on the true class). It assigns class labels (e.g. spam = 1 or spam = 0). In other words, Naive Bayes models the probabilities of the presence of each _word_, given that we have a spam email, and given that we have a non-spam email.  Then it combines them using Bayes Theorem (again, as described in the book).\n",
    "\n",
    "Using this model in sklearn works just the same as the others we've seen ([More details here..](http://scikit-learn.org/stable/modules/naive_bayes.html))\n",
    "\n",
    "- Choose the model\n",
    "- Fit the model (Train)\n",
    "- Predict with the model (Train or Test or Use data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "# Naive Bayes has an alpha parameter, which operates exactly like the lambda parameter for Logistic Regression\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train_counts, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"AUC on the count TRAIN data = %.3f\" % metrics.roc_auc_score(Y_train, model.predict(X_train_counts)))\n",
    "print (\"AUC on the count TRAIN data = %.3f\" % metrics.roc_auc_score(Y_train, model.predict_proba(X_train_counts)[:, 1]))\n",
    "\n",
    "print (\"AUC on the count TEST data = %.3f\" % metrics.roc_auc_score(Y_test, model.predict(X_test_counts)))\n",
    "print (\"AUC on the count TEST data = %.3f\" % metrics.roc_auc_score(Y_test, model.predict_proba(X_test_counts)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The past few weeks we have seen that many of the models we are using have different complexity control parameters that can be tweaked. In naive Bayes, the parameter that is typically tuned is the Laplace smoothing value **`alpha`**.\n",
    "\n",
    "Also, there are other versions of naive Bayes:\n",
    "\n",
    "1. **Multinomial naive Bayes (MNB):** This model handles count features and not just binary features. Sometimes MNB is used with binary presence/absence variables anyway (like word presence), even though that violates the model assumptions, because in practice it works well anyway.\n",
    "2. **Gaussian Naive Bayes (GNB):** This model considers likelihood of the features as Gaussian--and thus we can use it for continuous features.  Sometimes GNB and Bernoulli NB are combined when one has features of mixed types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include the false positives and true positives to the previous list\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test, model.predict_proba(X_test_tfidf)[:,1])\n",
    "\n",
    "tprs.append(tpr)\n",
    "fprs.append(fpr)\n",
    "roc_labels.append(\"Default BNB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Plot the list again: 3 Logistic Regressions vs Naive Bayes\n",
    "for fpr, tpr, roc_label in zip(fprs, tprs, roc_labels):\n",
    "    plt.plot(fpr, tpr, label=roc_label)\n",
    "\n",
    "plt.xlabel(\"fpr\")\n",
    "plt.ylabel(\"tpr\")\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim([0, .07])\n",
    "plt.ylim([.97, 1.001])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a counter\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "count_vectorizer.fit(X_train)\n",
    "\n",
    "# Transform to counter\n",
    "X_train_counts = count_vectorizer.transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Model\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train_counts, Y_train)\n",
    "\n",
    "## Include the false positives and true positives to the previous list\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test, model.predict_proba(X_test_counts)[:,1])\n",
    "\n",
    "tprs.append(tpr)\n",
    "fprs.append(fpr)\n",
    "roc_labels.append(\"Bigram BNB\")\n",
    "\n",
    "\n",
    "print (\"Area under the ROC curve on test data = %.3f\" % metrics.roc_auc_score(Y_test, model.predict(X_test_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the features are in a bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, k in enumerate(zip(count_vectorizer.vocabulary_.keys(), count_vectorizer.vocabulary_.values())):\n",
    "    print (k)\n",
    "    if i >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Plot the list again: 3 Logistic Regressions vs Naive Bayes\n",
    "for fpr, tpr, roc_label in zip(fprs, tprs, roc_labels):\n",
    "    plt.plot(fpr, tpr, label=roc_label)\n",
    "\n",
    "plt.xlabel(\"fpr\")\n",
    "plt.ylabel(\"tpr\")\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim([0, .07])\n",
    "plt.ylim([.97, 1.001])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Extra Credit Assignment (5 points)\n",
    "\n",
    "In the `data/` directory here, there is a zip file called `sentiment_analysis.zip`. The file contains two files: one called \"training.txt\" and another one called \"testing.txt\". Each file contains a number of lines. Each line is separated by a tab value. The first part of the line is a piece of text (a \"review\"). The second part of the ilne is a binary value, 0 or 1, indicating whether the review is negative (0) or positive (1).\n",
    "\n",
    "Using code that you see in this notebook and, possibly, code from previous notebooks, do the following:\n",
    "\n",
    "* Train 4 different classifiers on the contents of the \"training.txt\" file.\n",
    "* Test each of the 4 different classifiers on the contents of the \"testing.txt\" file.\n",
    "\n",
    "\n",
    "Evaluate your models as follows:\n",
    "- Create an ROC plot that contains the curves of **all** the 4 different models, when focusing on predicting the positive class.\n",
    "- Create an ROC plot that contains the curves of **all** the 4 different models, when focusing on predicting the negative class.\n",
    "\n",
    "Make sure that in your plots you specify which model corresponds to which line.\n",
    "\n",
    "Give a brief description of the results and say which model you would pick. \n",
    "\n",
    "\n",
    "**Deadline:** Sunday, March 31, 2019 11:59:59pm. I will create an assignment on Classes, where you can submit your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

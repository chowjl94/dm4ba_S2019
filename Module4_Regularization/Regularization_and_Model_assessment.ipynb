{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining for Business Analytics\n",
    "\n",
    "## Complexity control via Regularization\n",
    "\n",
    "Spring 2019 - Prof. George Valkanas\n",
    "\n",
    "Material based on content courtesy of Prof. Foster Provost\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all of the packages we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "\n",
    "from dstools import data_tools\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity Control, revisited\n",
    "\n",
    "One of the most important fundamental principles underlying data science, the basis for machine learning, is *complexity control*.  We must manage the tension between allowing ourselves to fit complex patterns in the data (a good thing), and the tendency to fit idiosyncracies in a particular data set--things that do not generalize (a bad thing).  So we attempt to **control model complexity**.\n",
    "\n",
    "One way to do that is just not to allow our data mining procedure to fit complex models in the first place.  Using _few_ features is a way to do this (but which ones?). Is there a way to address the problem in a <i>data-driven fashion</i> ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity Control for Decision Tree\n",
    "\n",
    "Given our previous discussion(s), what is it that makes Decision Trees _complex_ ? What did we do to address this problem?\n",
    "\n",
    "<br/>\n",
    "<img src=\"images/deep_tree.png\" height=100% width=100% />\n",
    "\n",
    "\n",
    "We run our accuracy on our train and test set and get the following results:\n",
    "<img src=\"images/d20_tree_acc.png\" />\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there are situations where _few_ features are not a realistic option -- text classification is an example. So let's consider the case where we have larger feature sets or non-linearities which might add important predictive power.\n",
    "\n",
    "_Again_, visualizing a high dimensional space is not practically feasible. We will illustrate the concepts using a two-dimensional dataset, but the idea applies in general, no matter how many features we have.\n",
    "\n",
    "This notebook comes with a CSV file that is a reduced version of the [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set). The dataset is within the `\"data/\"` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowersDf = pd.read_csv('data/flowers_data.csv')\n",
    "data_tools.Decision_Surface(flowersDf, flowersDf[\"Class\"], model=None )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like an easy dataset, with linearly separable classes. Let's use a Logistic Regression Classifier. We will also plot the **Decision Boundary**, i.e., the line that according to our model separates the two classes.\n",
    "\n",
    "To plot the decision boundary, we need to first write the code for it. We'll start with that. Afterwards, we will train our classifier and plot the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Decision_Boundary(features, target, model):\n",
    "    # Get bounds\n",
    "    x_min, x_max = features.min()[0], features.max()[0]\n",
    "    y_min, y_max = features.min()[1], features.max()[1]\n",
    "\n",
    "    # Create a mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max,0.01), np.arange(y_min, y_max,0.01))\n",
    "    meshed_data = pd.DataFrame(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    Z = np.ones(xx.shape)\n",
    "    if model is not None:\n",
    "        Z = model.predict(meshed_data).reshape(xx.shape)\n",
    "\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.xlabel(features.columns[0])\n",
    "    plt.ylabel(features.columns[1])\n",
    "\n",
    "    color = data_tools.Color_Data_Points(target)\n",
    "    cs = plt.contour(xx, yy, Z)\n",
    "    plt.scatter(features[features.columns[0]], features[features.columns[1]], color=color, edgecolor='black' )\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = flowersDf[[\"Sepal Width\", \"Petal Width\"]]  # Let's just get the features of interest\n",
    "labels = flowersDf[\"Class\"]  # The values for the target variable are often called \"labels\"\n",
    "\n",
    "logRegClf = LogisticRegression(C=100000000)\n",
    "logRegClf.fit( features, labels )\n",
    "Decision_Boundary(features, labels, model=logRegClf )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the Decision Boundary changes as we change the dataset. We will first visualize the datasets and then do the same for the respective decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will take an existing dataframe and append a single value to the end.\n",
    "# It will return a new dataset\n",
    "def append_data(dframe, values):\n",
    "    new_dframe = dframe.copy()\n",
    "    new_dframe.loc[-1] = values\n",
    "    new_dframe.index += 1\n",
    "    new_dframe.sort_index()\n",
    "    return new_dframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include a new data point to the original data\n",
    "extraFlower1 = append_data(flowersDf, [3, 1, 0])\n",
    "\n",
    "# Include a new data point to the original data\n",
    "extraFlower2 = append_data(flowersDf, [4, 0.7, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# visualize the information that appears in the dataframe\n",
    "def visualize_flowers( df, model=None ):\n",
    "    features = df[[\"Sepal Width\", \"Petal Width\"]]  # Let's just get the features of interest\n",
    "    labels = df[\"Class\"]  # The values for the target variable are often called \"labels\"\n",
    "\n",
    "    # If we were given an actual model, we are training it here.\n",
    "    if model is not None:\n",
    "        model.fit(features, labels)\n",
    "\n",
    "    Decision_Boundary(features, labels, model)\n",
    "    # data_tools.Decision_Surface(features, labels, model=model)\n",
    "    return\n",
    "\n",
    "\n",
    "# Setting the overall size (width, height) of the display area\n",
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "#Let's plot the three datasets side by side\n",
    "plt.subplot(1,3,1)\n",
    "visualize_flowers( flowersDf )\n",
    "\n",
    "#Let's plot the three datasets side by side\n",
    "plt.subplot(1,3,2)\n",
    "visualize_flowers( extraFlower1 )\n",
    "\n",
    "#Let's plot the three datasets side by side\n",
    "plt.subplot(1,3,3)\n",
    "visualize_flowers( extraFlower2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the Decision Boundary changes for these datasets. Again, we will be learning a Logistic Regression Function for all three cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We just \"create\" the model parameters here. We do *not* train it here.\n",
    "# We will be training it inside the \"visualize_flowers\" method\n",
    "untrainedLogReg = LogisticRegression(C=100000000)\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "#Let's plot the three datasets side by side\n",
    "plt.subplot(1,3,1)\n",
    "visualize_flowers( flowersDf, untrainedLogReg )\n",
    "\n",
    "#Let's plot the three datasets side by side\n",
    "plt.subplot(1,3,2)\n",
    "visualize_flowers( extraFlower1, untrainedLogReg )\n",
    "\n",
    "#Let's plot the three datasets side by side\n",
    "plt.subplot(1,3,3)\n",
    "visualize_flowers( extraFlower2, untrainedLogReg )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same thing with an SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We just \"create\" the model parameters here. We do *not* train it here.\n",
    "# We will be training it inside the \"visualize_flowers\" method\n",
    "untrainedSVM = LinearSVC()\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "#Let's plot the three datasets side by side\n",
    "plt.subplot(1,3,1)\n",
    "visualize_flowers( flowersDf, untrainedSVM )\n",
    "\n",
    "#Let's plot the three datasets side by side\n",
    "plt.subplot(1,3,2)\n",
    "visualize_flowers( extraFlower1, untrainedSVM )\n",
    "\n",
    "#Let's plot the three datasets side by side\n",
    "plt.subplot(1,3,3)\n",
    "visualize_flowers( extraFlower2, untrainedSVM )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity control for logistic regression (regularization)\n",
    "\n",
    "Recall that when training a logistic regression classifier, we try to find the set of weights, $\\textbf{w}$, that best fit the data based on some objective function. In this case, let's call our objective function $g()$, which means that we want $\\arg\\max_\\textbf{w} g(\\textbf{x}, \\textbf{w})$.\n",
    "\n",
    "What happens in the Logistic Regression case is that our model is trying to find the best $\\textbf{w}$, even if \"outliers\" are accommodated in the process! Note that the model (by itself) **does not** know anything about outliers; _we_ perceive these data points as outliers. \n",
    "\n",
    "Mathematically, these outliers \"force\" $\\textbf{w}$ to take very high values for some dimensions.  As our previous example showed, we do not want that to happen.  Instead, we want to **penalize** such situations -- and, more generally, penalize our \"fit\" of the data as it gets more complex. This is achieved by adding a \"penalty term\" into the objective function, using a **\"regularization parameter\"** $\\lambda$ which controls how much importance our optimization procedure should place on the fit vs. the penalty. The new optimization formula becomes:\n",
    "\n",
    "$\\arg\\max_\\textbf{w} g(\\textbf{x}, \\textbf{w}) - \\lambda \\cdot \\text{penalty}(\\textbf{w})$.\n",
    "\n",
    "The two most common type of regularization in logistic regression are the so-called $L_1$ and $L_2$ regularizations, which simply use the sum of the weights (w) and the sum of the squares of the weights, respectively, as the penalty.\n",
    "\n",
    "\n",
    "**Extra note**: Several software packages (sklearn being one of them) represent the \"regularization parameter\" as `c`, which is usually $\\frac{1}{\\lambda}$. Therefore, smaller values of `c` lead to larger complexity penalties, and vice versa.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the above example with different values of regularization.\n",
    "\n",
    "_Remember_: Sklearn uses the `c` parameter, which is $\\frac{1}{\\lambda}$. So, lower values of `c` incur a _higher_ penalty (\"more\" regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization values\n",
    "cValues = [100000000, 10000, 1, 0.01]\n",
    "\n",
    "for c in cValues:\n",
    "\n",
    "    # Create the model with the desired parameters\n",
    "    untrainedLogReg = LogisticRegression(C=c, solver='lbfgs')\n",
    "\n",
    "    plt.figure(figsize=(15,4))\n",
    "\n",
    "    #Let's plot the three datasets side by side\n",
    "    plt.subplot(1, 3, 1)\n",
    "    visualize_flowers( flowersDf, untrainedLogReg )\n",
    "    plt.title(\"Decision Boundary c=%f\" % c)\n",
    "\n",
    "    #Let's plot the three datasets side by side\n",
    "    plt.subplot(1, 3, 2)\n",
    "    visualize_flowers( extraFlower1, untrainedLogReg )\n",
    "    plt.title(\"Decision Boundary c=%f\" % c)\n",
    "\n",
    "    #Let's plot the three datasets side by side\n",
    "    plt.subplot(1, 3, 3)\n",
    "    visualize_flowers( extraFlower2, untrainedLogReg )\n",
    "    plt.title(\"Decision Boundary c=%f\" % c)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what happens when we change the type of regularization that we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cValue = 10.0\n",
    "for regularization in [\"L1\", \"L2\"]:\n",
    "\n",
    "    # We have ommitted the \"solver\" parameter here\n",
    "    untrainedLogReg = LogisticRegression(penalty=regularization.lower(), C=cValue)\n",
    "\n",
    "    plt.figure(figsize=(15,4))\n",
    "\n",
    "    #Let's plot the three datasets side by side\n",
    "    plt.subplot(1, 3, 1)\n",
    "    visualize_flowers( flowersDf, untrainedLogReg )\n",
    "    plt.title(\"Decision Boundary c=%f, reg=%s\" % (cValue, regularization))\n",
    "\n",
    "    #Let's plot the three datasets side by side\n",
    "    plt.subplot(1, 3, 2)\n",
    "    visualize_flowers( extraFlower1, untrainedLogReg )\n",
    "    plt.title(\"Decision Boundary c=%f, reg=%s\" % (cValue, regularization))\n",
    "\n",
    "    #Let's plot the three datasets side by side\n",
    "    plt.subplot(1, 3, 3)\n",
    "    visualize_flowers( extraFlower2, untrainedLogReg )\n",
    "    plt.title(\"Decision Boundary c=%f, reg=%s\" % (cValue, regularization))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What $\\lambda$ value to use?\n",
    "\n",
    "Can you think of a way to find a good value for the regularization parameter $\\lambda$? What did we do for Decision Trees?\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

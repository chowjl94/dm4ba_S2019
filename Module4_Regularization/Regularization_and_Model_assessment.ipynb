{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining for Business Analytics\n",
    "\n",
    "## Overfitting Avoidance\n",
    "\n",
    "Spring 2019 - Prof. George Valkanas\n",
    "\n",
    "Material based on content courtesy of Prof. Foster Provost\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all of the packages we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "from dstools import data_tools\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity Control, revisited\n",
    "\n",
    "One of the most important fundamental principles underlying data science, the basis for machine learning, is *complexity control*.  We must manage the tension between allowing ourselves to fit complex patterns in the data (a good thing), and the tendency to fit idiosyncracies in a particular data set--things that do not generalize (a bad thing).  So we attempt to control complexity.  One way to control complexity is just not to allow our data mining procedure to fit complex models in the first place.  Using _few_ features is a way to do this (but which ones?).\n",
    "\n",
    "Is there a way to address the problem in a <i>data-driven fashion</i> ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity Control for Decision Tree\n",
    "\n",
    "So, we've built our Decision Tree classifier for our dataset and we've allowed it to grow (a lot).\n",
    "<br/>\n",
    "<img src=\"images/deep_tree.png\" height=100% width=100% />\n",
    "\n",
    "\n",
    "We run our accuracy on our train and test set and get the following results:\n",
    "<img src=\"images/d20_tree_acc.png\" />\n",
    "***\n",
    "\n",
    "**Question:** What is the problem that we are seeing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there are situation where _few_ features are not an option -- text classification is an example. So let's consider the case where we have larger feature sets or non-linearities which might add important predictive power.\n",
    "\n",
    "So let's create some example data that include non-linear features, using our script \"data_tools\". (You should be familiar with this after last class!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some data\n",
    "target_name, variable_names, data, Y = data_tools.create_data()\n",
    "\n",
    "# Grab the predictors with sufficient complexity (up to 3rd-order interaction terms)\n",
    "X = data_tools.X(complexity=3)\n",
    "\n",
    "X.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity control for logistic regression (regularization)\n",
    "\n",
    "Recall that when fitting a logistic regression classifier, we try to find the set of weights, $\\textbf{w}$, that maximize the fit to the data, based on some fit (objective) function. In this case, let's call our objective function $g()$, which means that we want $\\arg\\max_\\textbf{w} g(\\textbf{x}, \\textbf{w})$.\n",
    "\n",
    "Since we are trying to find the set of weight without too much complexity, when we perform **regularization** we **penalize** our fit as it gets more complex.  This is achieved by adding a \"penalty term\" into the objective function, and using a \"regularization parameter\" $\\lambda$ (also sometimes represented as `c`, which is usually $\\frac{1}{\\lambda}$ so smaller values of `c` lead to larger complexity penalties) to specify how much importance our optimization procedure should place on the fit vs. the penalty:\n",
    "\n",
    "$\\arg\\max_\\textbf{w} g(\\textbf{x}, \\textbf{w}) - \\lambda \\cdot \\text{penalty}(\\textbf{w})$.\n",
    "\n",
    "The two most common type of regularization in logistic regression are the so-called $L_1$ and $L_2$ regularizations, which simply use the sum of the weights (w) and the sum of the squares of the weights, respectively, as the penalty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot different regularization values for L1 and L2 regularization\n",
    "for regularization in ['L2', 'L1']:\n",
    "    # Get a table of the model coefficients\n",
    "    coefs = pd.DataFrame(columns=['C'] + list(X.columns))\n",
    "    \n",
    "    # Print what we are doing\n",
    "    print (\"\\nFitting with %s regularization: \\n\" % regularization)\n",
    "    position = 0\n",
    "    \n",
    "    # Try some regularization values\n",
    "    for i in range(-6,3):\n",
    "        # Modeling\n",
    "        c = np.power(10.0, i)  # Higher values means LESS regularization\n",
    "        model = LogisticRegression(penalty=regularization.lower(), C=c)\n",
    "        model.fit(X, Y)\n",
    "        \n",
    "        # Plotting\n",
    "        position += 1\n",
    "        plt.subplot(3, 3, position)\n",
    "        data_tools.Decision_Surface(X, Y, model, size=0.25)\n",
    "        plt.title(\"C = \" + str(np.power(10.0, i)))\n",
    "        \n",
    "        # Update coefficient table\n",
    "        coefs.loc[i] = [c] + list(model.coef_[0])\n",
    "    # Print and plot\n",
    "    #print (coefs.to_string(index=False))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature's weights with L1/L2  and normalization\n",
    "\n",
    "Let's take a look to the different values of our weights with each type of penalty but first, let's understand **normalization**. \n",
    "\n",
    "What is normalization? Why do we need normalization? Each time we work with data, it is very important to consider the \"scale\" of the features. Some features might have distinct values from 1 to 1000, and other features might have values from 0 to 1. As many different data science/machine learning methods compare data along different dimensions, it can often be important to make sure the dimensions are comparable.\n",
    "\n",
    "To do this re-scaling there are are many approaches, the most common being:\n",
    "\n",
    "- _Normalization_ : we rescale our data so that the features have unit norms  \n",
    "- _Standardization_ : we rescale our data acting as if each features is normally distributed (Gaussian with zero mean and unit variance)\n",
    "- _Scaling to a range_ : we rescale our data based on the minimum and maximum value of each feature \n",
    "\n",
    "\n",
    "( sklearn has a built-in function to help us re-scaling our data -- see below)\n",
    "\n",
    "**Let's take a look at the data before and after re-scaling.**\n",
    "\n",
    "Before re-scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import scale\n",
    "X_scaled = pd.DataFrame(scale(X, axis=0, with_mean=True, with_std=True, copy=True), columns = X.columns.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After re-scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just format our output, to make it look nicer\n",
    "summary = X_scaled.describe()\n",
    "for column in summary: \n",
    "    summary[column] = summary[column].apply(lambda x: round(x,1))\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the weights as we change the regularization\n",
    "\n",
    "You should be able to notice that the result of **no regularization** (which in this case **for sklearn, it means a high value of C**, let's say C=1000) shows the following (and see the graph below--ignore the x-axis for the moment and just look at the y values): \n",
    "\n",
    "- Humor^2 has a weight close to zero\n",
    "- Number_of_pets (orange line) has a greater weight than humor (blue line)\n",
    "- Humor^3 has a weight close to five (red line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL2 = {}\n",
    "MODEL1 = {}\n",
    "MODEL0 = {}\n",
    "\n",
    "for i in np.arange(-5, 5,0.5):  #go through a bunch of ascending regularization parameters\n",
    "    \n",
    "    LR_l1 = LogisticRegression(C=10**i, penalty = 'l1')\n",
    "    LR_l1.fit(X_scaled, Y)\n",
    "    MODEL1[10**i] = LR_l1.coef_[0] \n",
    "    \n",
    "    LR_l2 = LogisticRegression(C=10**i, penalty = 'l2', solver='lbfgs')\n",
    "    LR_l2.fit(X_scaled, Y)\n",
    "    MODEL2[10**i] = LR_l2.coef_[0]\n",
    "\n",
    "    LR = LogisticRegression(C=1000)\n",
    "    LR.fit(X_scaled, Y)\n",
    "    MODEL0[10**i] = LR.coef_[0]\n",
    "\n",
    "COLUMN_NAMES = X_scaled.columns.values\n",
    "path_penalty2 = pd.DataFrame(MODEL2, index=COLUMN_NAMES ).transpose()\n",
    "path_penalty1= pd.DataFrame(MODEL1, index=COLUMN_NAMES ).transpose()\n",
    "path_penalty0= pd.DataFrame(MODEL0, index=COLUMN_NAMES ).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now plot the \"regularization paths\" -- how the model parameters change with regularization\n",
    "#  First, without regularization at all, let's see what the parameters would be\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for f in COLUMN_NAMES : plt.plot(path_penalty0.index.values, path_penalty0[[f]], label=f)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.15, 1), ncol=1, prop={'size':10})\n",
    "plt.title('No Regularization')\n",
    "plt.xlabel('Regularization C (higher values means less regularization)')\n",
    "plt.ylabel('Feature Weight')\n",
    "plt.xscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Once we plot the models with different penalties, we can see how the model weights change with more complexity control.\n",
    "\n",
    "Also, you should be able to see that the different types of penalty (l1 or l2) create **different changes**:\n",
    "    \n",
    "- Humor^2 has a weight close to zero almost all the time for L1, but this is not the case with L2 \n",
    "- Number_of_pets (orange line) has a smaller weight than humor (blue line) with more regularization (values of penalty below 10^0=1)\n",
    "- The weight of Humor^3 decreases in different ways (paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now plot the \"regularization paths\" -- how the model parameters change with regularization\n",
    "# Now, let's actually plot the paths for L1 and L2 regularization\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for f in COLUMN_NAMES : plt.plot(path_penalty1.index.values, path_penalty1[[f]], label=f)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.15, 1), ncol=1, prop={'size':10})\n",
    "plt.title('L1 Regularization')\n",
    "plt.xlabel('Regularization C (higher values means less regularization)')\n",
    "plt.ylabel('Feature Weight')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for f in COLUMN_NAMES : plt.plot(path_penalty2.index.values, path_penalty2[[f]], label=f)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.15, 1), ncol=1, prop={'size':10})\n",
    "plt.title('L2 Regularization')\n",
    "plt.xlabel('Regularization C (higher values means less regularization)')\n",
    "plt.ylabel('Feature Weight')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves for model assessment\n",
    "\n",
    "We now can fit the data and deal with generalization and complexity control. Now we can start comparing different models, assessing classification accuracy.\n",
    "\n",
    "You started doing this in your last homework, **tuning parameters** of a tree-structured model to improve the accuracy of the classifier (in this case, for the churn example). \n",
    "\n",
    "In the last homework, you used the following options: \n",
    "\n",
    "1. max_depth: To create trees with a specific maximum depth value (leaf nodes with a limited maximum number of edges from the root).\n",
    "2. min_samples_split: To create a tree with a specific \"minimum number of samples\" required to _split_ an internal node.\n",
    "3. min_samples_leaf: To create a tree with a specific \"minimum number of samples\" required to be at a _leaf_ node.\n",
    "\n",
    "For those cases, you compared accuracies, using a unique **sample size** that we gave you.\n",
    "\n",
    "But very often we also want to assess the relationship between how much data we are using to train the models, and the generalization performance we achieve.  For example, do we have a good idea whether we should invest in acquiring more training data? The only way to answer this question is again, experiment with different sample sizes. The main way to do this assessment is via **_learning curves_**: analyze the change of the generalization performance (accuray on the holdout data, in this case) based on different sizes of the training set.\n",
    "\n",
    "What would we expect to see? Holding everything else fixed, the generalization should be better with more training data, up until a certain point. Then, more data won't increase generalization performance. \n",
    "\n",
    "Thus, learning curves will help to determine at least 2 things:\n",
    "\n",
    "- We can see which model performs better or worse for each sample size (e.g. Decision Tree vs Logistic Regression)\n",
    "- We can get a sense of whether getting more data (or using less) will improve (or not degrade) generalization.\n",
    "\n",
    "\n",
    "<img src=\"dstools/learning.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Often learning curves use a log scale on the x-axis, which I didn't do in that graph just presented.  Imagine above how that would give more detail on the interesting part of the curves.  Try it!   [You can see how to use a log scale in matplotlib in the code above.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on\n",
    "\n",
    "Let's start to actually work with the **predictions** of a learned model.  Use Logistic Regression with L2 regularization with c = 0.1.\n",
    "\n",
    "Create a Data Frame with 2 columns: one with the predicted-label ($\\hat{y}$) and one with the actual label (y). \n",
    "\n",
    "- Count the number of people (rows) who got success=\"1\" in the model ($\\hat{y}$) when their actual label is \"1\".\n",
    "- Count the number of people (rows) who got success=\"0\" in the model ($\\hat{y}$) when their actual label is \"1\".\n",
    "\n",
    "- Compute some stats looking at these errors in different ways.\n",
    "\n",
    "**When you read Chapters 7 & 8, think about this.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

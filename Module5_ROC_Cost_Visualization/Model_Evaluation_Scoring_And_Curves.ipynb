{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining for Business Analytics\n",
    "\n",
    "## Model Evaluation Measures - Pt. 2\n",
    "\n",
    "Spring 2019 - Prof. George Valkanas\n",
    "\n",
    "Material based on content courtesy of Prof. Foster Provost\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Just as a reminder, a confusion matrix looks like this\n",
    "\n",
    "<br/>\n",
    "<table width=\"60%\">\n",
    "    <tbody>\n",
    "        <tr style=\"background: rgba(255, 255, 255, 0.1)\">\n",
    "            <td colspan=\"2\" width=\"30%\"></td>\n",
    "            <td colspan=\"2\" style=\"border: 2px solid black; background: white; text-align: center\" ><b>True Class</b></td>\n",
    "        </tr>\n",
    "        <tr style=\"background: rgba(255, 255, 255, 0.1)\">\n",
    "            <td colspan=\"2\"></td>\n",
    "            <td style=\"border: 2px solid black; background: #E8E8E8; text-align: center\"><b>Positive (1)</b></td>\n",
    "            <td style=\"border: 2px solid black; background: #E8E8E8; text-align: center\"><b>Negative (0)</b></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=\"2\" style=\"border: 2px solid black; background: white; text-align: center\"><b>Predicted<br/>Class</b></td>\n",
    "            <td style=\"border: 2px solid black; background: #E8E8E8; text-align: center\"><b>Positive<br/>(\"Yes\")</b></td>\n",
    "            <td style=\"border: 2px solid black; background: lightgreen; text-align: center\">\n",
    "                <b>True Positive (TP)</b>\n",
    "            </td>\n",
    "            <td style=\"border: 2px solid black; background: #ff9999; text-align: center\">\n",
    "                <b>False Positive (FP)</b>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 2px solid black; background: #E8E8E8; text-align: center; text-align: center\">\n",
    "                <b>Negative<br/>(\"No\")</b>\n",
    "            </td>\n",
    "            <td style=\"background: #ff9999; border: 2px solid black; text-align: center\">\n",
    "                <b>False Negative (FN)</b>\n",
    "            </td>\n",
    "            <td style=\"background: lightgreen; border: 2px solid black; text-align: center\">\n",
    "                <b>True Negative (TN)</b>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already seen the _Cumulative Response Curve_ (CRC) and the process that we use to generate it. There are several other curves that we can use to evaluate a model. Some of them are quite similar to the CRC, whereas others differ more.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Additional Measures\n",
    "\n",
    "To produce the extra curves, we must first introduce some additional measures, which we can build from our confusion matrix.\n",
    "\n",
    "* **True Positive Rate (TPR):** Also known as <u>_recall_</u> is the portion of actually true instances (\"truely\" to the positive class) that were classified / captured / identified as such. To compute TPR, we use the \"Positive (1)\" column of our confusion matrix. Formally:\n",
    "\n",
    "$$ TPR = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "<br/>\n",
    "\n",
    "* **False Positive Rate (FPR):** Roughly speaking, how likely we are to _misclassify_ a \"truely\" negative instance as a positive one. To compute FPR, we use the \"Negative (0)\" column of our confusion matrix. Formally:\n",
    "\n",
    "$$ FPR = \\frac{FP}{FP + TN}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VERY IMPORTANT**: As you can see from the previous quantities, TPR and FPR use values from our _confusion matrix_. What does that mean, with respect to a classifier? Recall that our models are _scoring models_ !\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ROC  \n",
    "\n",
    "\n",
    "_Not to be confused with [The Rock](https://www.imdb.com/name/nm0425005/)!_\n",
    "\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) Curve uses the two measures that we introduced above: **FPR** and **TPR**.\n",
    "It helps us to visualize the _trade-offs_ between the **opportunity for benefits** (via true positives on the y-axis) and the **possibility of costs** (via false positives on the x-axis).\n",
    "\n",
    "The ROC _also_ expects our predictions to be _ordered_ according to their probability estimates. Let's briefly examine how it works.\n",
    "\n",
    "<img src=\"images/ROC1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "The four corners of an ROC curve:\n",
    "\n",
    "* **(0, 0):** Represents the strategy of never issuing a positive classification\n",
    "* **(0, 1):** Represents **perfect classification**, i.e. everything is classified to the class it really belongs! \n",
    "* **(1, 0):** We somehow managed to get _everything_ wrong!?\n",
    "* **(1, 1):** Always issue positive classification (example: \"everyone's pregnant!\")\n",
    "\n",
    "The diagonal line between **(0, 0)** and **(1, 1)** represents a randomly guessing \"classifier\" (e.g., flipping a weighted coin).\n",
    "\n",
    "- _Provost, Foster, and Tom Fawcett. Data Science for Business: _\n",
    "  _What you need to know about data mining and data-analytic thinking. O'Reilly Media, Inc., 2013._\n",
    "\n",
    "\n",
    "<img src=\"images/ROC2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC\n",
    "\n",
    "Finally, we can measure the _Area Under the (ROC) Curve_ (ROC AUC), which is the fraction of the total area below our classifier's ROC curve.\n",
    "_Remember:_ Despite its simplicity, you should approach the AUC just like any other evaluation measures: whether it's the right thing to use depends on your business application and domain!\n",
    "\n",
    "Let's go back to our mailing campaign example and compute the AUC there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv(\"data/mailing.csv\")  # Load the data\n",
    "original.head()  # Let's take a look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummysize the data\n",
    "Check our other notebook for details why we are doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original.copy()\n",
    "for field in ['rfaa2', 'pepstrfl']:  # Do the same thing for the two fields of interest\n",
    "    # Go through each possible value \n",
    "    for value in data[field].unique():\n",
    "        # Create a new binary field\n",
    "        data[field + \"_\" + value] = pd.Series(data[field] == value, dtype=int)\n",
    "\n",
    "    # Drop the original field\n",
    "    data = data.drop([field], axis=1)\n",
    "    \n",
    "# Let's look at the data again, after the modifications\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's createa a _new_ feature that may be _more_ useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"data\" dataframe contains everything together.\n",
    "# Let's be picky this time w.r.t. the features that we'll use\n",
    "data['date_diff'] = 1 + (data['Lastdate'] - data['Firstdate'])\n",
    "X = data[['Income', 'Amount', 'rfaf2', 'glast', 'gavr', 'date_diff']]\n",
    "Y = data['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the features and labels into a training and testing subset. We'll use 75% for training and the rest for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep 75% of the data for training and 25% for testing. We have used this method before.\n",
    "X_mailing_train, X_mailing_test, Y_mailing_train, Y_mailing_test = train_test_split(X, Y, train_size=.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start simple, with a Logistic Regression classifier. These are the steps we must take:\n",
    "1. Train the classifier (on the training data)\n",
    "1. Use the classifier on the _test_ data and get the _probabilities_ that an instance belongs to the _positive_ class.\n",
    "1. Get the $FPR$ and $TPR$ quantities to generate the AUC value (definitions earlier in this notebook). To do so, use:\n",
    "    1. The probabilities returned by the classifier in the previous step.\n",
    "    1. The _true labels_ of the test ( **why the test set** ? )\n",
    "    1. The `metrics.roc_curve()` method\n",
    "1. Use the `roc_auc_score()` method to generate the AUC value\n",
    "1. Plot the ROC curve, with $FPR$ on the x-axis and $TPR$ on the y-axis.\n",
    "    1. For convenience, we'll "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_mailing_train, Y_mailing_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: For each instance in the TEST DATA, get the probability that it belongs to the POSITIVE class (LEAVE=1)\n",
    "Y_test_probability_1 = model.predict_proba(X_mailing_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use the roc_curve method to generate FPR and TPR\n",
    "fpr, tpr, thresholds = metrics.roc_curve(Y_mailing_test, Y_test_probability_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compute the AUC for the ROC\n",
    "auc = metrics.roc_auc_score(Y_mailing_test, Y_test_probability_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Plot the ROC curve. Report the AUC in the legend\n",
    "plt.plot(fpr, tpr, label=\"AUC = %.2f\" % round(auc, 2))\n",
    "plt.xlabel(\"False positive rate (fpr)\")\n",
    "plt.ylabel(\"True positive rate (tpr)\")\n",
    "plt.plot([0,1], [0,1], 'k--', label=\"Random\")\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Generate the same plot for different classifiers. Do you get better performance? Worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Lift Curve\n",
    "\n",
    "The ROC Curve has been a fundamental tool in evaluating the performance of classifiers, for a number of reasons. (can you think of any?).\n",
    "\n",
    "A potential downside is that **FPR** and **TPR** may be tricky quantities to express / explain in a business setting. The CRC is perhaps more \"approachable\".\n",
    "\n",
    "Another type of curve that we often user is known as the **Lift Curve**. It tells us _how much better_ is one model with respect to another. To compute our Lift Curve, we will start with our CRC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's switch gears again and see what happens if we use all the features\n",
    "X = data.drop(['class'], axis=1)\n",
    "\n",
    "# Keep 75% of the data for training and 25% for testing. We have used this method before.\n",
    "X_mailing_train, X_mailing_test, Y_mailing_train, Y_mailing_test = train_test_split(X, Y, train_size=.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's create a method that trains and returns the CRC of a clasifier\n",
    "\n",
    "def train_and_compute_crc( model, x_train, y_train, x_test, y_test):\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Let's get the probabilities. FOCUS ON THE POSITIVE CLASS\n",
    "    probabilities = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    # Create a dataframe that we can conveniently manipulate\n",
    "    model_df = pd.DataFrame(list(zip(probabilities, y_test)), columns=[\"PROBABILITY\", \"TRUE_CLASS\"])\n",
    "\n",
    "    # Sort the dataframe rows by the PROBABILITY\n",
    "    model_df_sorted = model_df.sort_values(by=['PROBABILITY'], ascending=False)\n",
    "\n",
    "    # Compute the CUMULATIVE correct responses up until the\n",
    "    return model_df_sorted[\"TRUE_CLASS\"].cumsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a few classifiers here and try to plot everything together.\n",
    "model = LogisticRegression(C=100)\n",
    "logReg10_crc = train_and_compute_crc( model, X_mailing_train, Y_mailing_train, X_mailing_test, Y_mailing_test )\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=15)\n",
    "dec_tree_crc = train_and_compute_crc( model, X_mailing_train, Y_mailing_train, X_mailing_test, Y_mailing_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the x-axis compute the fraction of instances we have seen so far\n",
    "x_fraction = []\n",
    "for i in range(len(X_mailing_test)):\n",
    "    x_fraction.append( (i+1) / len(X_mailing_test) )\n",
    "x_fraction = np.array(x_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute the LIFT for the decision tree.\n",
    "# a) Normalize the CRC by the maximum value\n",
    "# b) Divide each value by the fraction of items we have seen so far\n",
    "dec_tree_lift = dec_tree_crc / np.max(dec_tree_crc)\n",
    "dec_tree_lift = dec_tree_lift / x_fraction\n",
    "\n",
    "\n",
    "# Compute the LIFT for the Logistic Regression.\n",
    "# a) Normalize the CRC by the maximum value\n",
    "# b) Divide each value by the fraction of items we have seen so far\n",
    "log_reg_lift = logReg10_crc / np.max(logReg10_crc)\n",
    "log_reg_lift = log_reg_lift / x_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's plot the information\n",
    "\n",
    "plt.figure(figsize=(10,7))  # Create a big enough plot\n",
    "\n",
    "plt.plot(x_fraction * 100, dec_tree_lift, label=\"Decision Tree\")  # Show the LIFT curve for the Decision Tree\n",
    "\n",
    "plt.plot(x_fraction * 100, log_reg_lift, label=\"Logistic Regression\")  # Show the LIFT curve for Logistic Regression\n",
    "\n",
    "plt.plot([0,100], [1,1], 'k--', label=\"Random\")\n",
    "plt.xlabel(\"Percentage of test instances (decreasing score)\")\n",
    "plt.ylabel(\"Lift (times)\")\n",
    "plt.title(\"Lift curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let's plot the information\n",
    "\n",
    "plt.figure(figsize=(10,7))  # Create a big enough plot\n",
    "\n",
    "plt.plot(x_fraction * 100, dec_tree_lift, label=\"Decision Tree\")  # Show the LIFT curve for the Decision Tree\n",
    "\n",
    "plt.plot(x_fraction * 100, log_reg_lift, label=\"Logistic Regression\")  # Show the LIFT curve for Logistic Regression\n",
    "\n",
    "plt.plot([0,100], [1,1], 'k--', label=\"Random\")\n",
    "plt.xlim(-0.3, 10)\n",
    "plt.xlabel(\"Percentage of test instances (decreasing score)\")\n",
    "plt.ylabel(\"Lift (times)\")\n",
    "plt.title(\"Lift curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What can you tell me by looking at the above graph?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profit Curves\n",
    "\n",
    "Finally, you should always remember that there is _evaluating the model_ and _evaluating the business_. The two may be linked in one way or another, but this connection has probably not been clear so far. So let's make this happen!\n",
    "\n",
    "Let's generate a simple matrix like the one below.\n",
    "**BE CAREFUL**: The matrix has the structure of a confusion matrix, _but it is not one!_\n",
    "\n",
    "**Question:** Can you see why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much do we gain by predicting the right individual ()\n",
    "indiv_benefit = 17\n",
    "\n",
    "# How much we lose if we predict the wrong individual (cost of sending out a flyer, without a positive response)\n",
    "indiv_loss = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cost_matrix = pd.DataFrame([[indiv_benefit, indiv_loss], [0, 0]], \n",
    "                           columns=['(True) p', '(True) n'], \n",
    "                           index=['[Predicted] Y', '[Predicted] N'])\n",
    "cost_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above matrix is a _profit matrix_. It says what we _gain_ by targeting a customer that the classifier said that we should target (predicted = \"Y\").\n",
    "\n",
    "We will basically combine our profit matrix with _each_ of our confusion matrices, i.e., for each threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a decision tree classifier with some parameters\n",
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=15)\n",
    "model.fit(X_mailing_train, Y_mailing_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every instance in the test set, take the probability it will belong to class 1\n",
    "model_probas = model.predict_proba(X_mailing_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the false positive rate, true positive rate, and all thresholds\n",
    "fpr, tpr, thresholds = metrics.roc_curve(Y_mailing_test, model_probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the baseline probability of being positive or negative in the data set?\n",
    "\n",
    "# The baseline is: # positive responses / # total number of responses\n",
    "p_p = np.sum(Y_mailing_test)/len(Y_mailing_test)\n",
    "\n",
    "p_n = 1 - p_p # According to our probabilities class...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To estimate our profit we use our TRUE POSITIVES and we want to convert these to \n",
    "actual_profit = cost_matrix['(True) p']['[Predicted] Y'] * tpr * p_p\n",
    "\n",
    "# To estimate our profit we use our FALSE POSITIVES\n",
    "actual_cost = cost_matrix['(True) n']['[Predicted] Y'] * fpr * p_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected profits:\n",
    "expected_profits = actual_profit + actual_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will use the thresholds here to compute how many users are above it\n",
    "n_targeted = []\n",
    "for t in thresholds:\n",
    "    above_threshold = model_probas >= t\n",
    "    above_thres_count = np.sum(above_threshold)\n",
    "    n_targeted.append( above_thres_count )\n",
    "\n",
    "# Turn these counts to percentages of users above the threshold\n",
    "n_targeted = np.array(n_targeted) / len(Y_mailing_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Size_targeted_pop = float(len(Y_mailing_test))\n",
    "\n",
    "# Plot the profit curve\n",
    "plt.plot(n_targeted, Size_targeted_pop*expected_profits)\n",
    "plt.xlabel(\"Percentage of users targeted\")\n",
    "plt.ylabel(\"Profit\")\n",
    "plt.title(\"Profits Curve\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # Let's compute the confusion matrix first.\n",
    "# # To do that we need the actual classes and the predicted ones\n",
    "# conf_mtx = metrics.confusion_matrix(Y_mailing_test, predictions, labels=[1, 0])\n",
    "\n",
    "# # Let's turn the confusion matrix into a DataFrame, to make it more presentable\n",
    "# conf_mtx_df = pd.DataFrame(conf_mtx.T, columns=[, ], index=[, '[Predicted] N'])\n",
    "# conf_mtx_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining for Business Analytics\n",
    "\n",
    "## Model Evaluation Measures - Pt. 2\n",
    "\n",
    "Spring 2019 - Prof. George Valkanas\n",
    "\n",
    "Material based on content courtesy of Prof. Foster Provost\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Just as a reminder, a confusion matrix looks like this\n",
    "\n",
    "<br/>\n",
    "<table width=\"60%\">\n",
    "    <tbody>\n",
    "        <tr style=\"background: rgba(255, 255, 255, 0.1)\">\n",
    "            <td colspan=\"2\" width=\"30%\"></td>\n",
    "            <td colspan=\"2\" style=\"border: 2px solid black; background: white; text-align: center\" ><b>True Class</b></td>\n",
    "        </tr>\n",
    "        <tr style=\"background: rgba(255, 255, 255, 0.1)\">\n",
    "            <td colspan=\"2\"></td>\n",
    "            <td style=\"border: 2px solid black; background: #E8E8E8; text-align: center\"><b>Positive (1)</b></td>\n",
    "            <td style=\"border: 2px solid black; background: #E8E8E8; text-align: center\"><b>Negative (0)</b></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=\"2\" style=\"border: 2px solid black; background: white; text-align: center\"><b>Predicted<br/>Class</b></td>\n",
    "            <td style=\"border: 2px solid black; background: #E8E8E8; text-align: center\"><b>Positive<br/>(\"Yes\")</b></td>\n",
    "            <td style=\"border: 2px solid black; background: lightgreen; text-align: center\">\n",
    "                <b>True Positive (TP)</b>\n",
    "            </td>\n",
    "            <td style=\"border: 2px solid black; background: #ff9999; text-align: center\">\n",
    "                <b>False Positive (FP)</b>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 2px solid black; background: #E8E8E8; text-align: center; text-align: center\">\n",
    "                <b>Negative<br/>(\"No\")</b>\n",
    "            </td>\n",
    "            <td style=\"background: #ff9999; border: 2px solid black; text-align: center\">\n",
    "                <b>False Negative (FN)</b>\n",
    "            </td>\n",
    "            <td style=\"background: lightgreen; border: 2px solid black; text-align: center\">\n",
    "                <b>True Negative (TN)</b>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already seen the _Cumulative Response Curve_ (CRC) and the process that we use to generate it. There are several other curves that we can use to evaluate a model. Some of them are quite similar to the CRC, whereas others differ more.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Additional Measures\n",
    "\n",
    "To produce the extra curves, we must first introduce some additional measures, which we can build from our confusion matrix.\n",
    "\n",
    "* **True Positive Rate (TPR):** Also known as <u>_recall_</u> is the portion of actually true instances (\"truely\" to the positive class) that were classified / captured / identified as such. To compute TPR, we use the \"Positive (1)\" column of our confusion matrix. Formally:\n",
    "\n",
    "$$ TPR = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "<br/>\n",
    "\n",
    "* **False Positive Rate (FPR):** Roughly speaking, how likely we are to _misclassify_ a \"truely\" negative instance as a positive one. To compute FPR, we use the \"Negative (0)\" column of our confusion matrix. Formally:\n",
    "\n",
    "$$ FPR = \\frac{FP}{FP + TN}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VERY IMPORTANT**: As you can see from the previous quantities, TPR and FPR use values from our _confusion matrix_. What does that mean, with respect to a classifier? Recall that our models are _scoring models_ !\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ROC  \n",
    "\n",
    "\n",
    "_Not to be confused with [The Rock](https://www.imdb.com/name/nm0425005/)!_\n",
    "\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) Curve uses the two measures that we introduced above: **FPR** and **TPR**.\n",
    "It helps us to visualize the _trade-offs_ between the **opportunity for benefits** (via true positives on the y-axis) and the **possibility of costs** (via false positives on the x-axis).\n",
    "\n",
    "The ROC _also_ expects our predictions to be _ordered_ according to their probability estimates. Let's briefly examine how it works.\n",
    "\n",
    "<img src=\"images/ROC1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "The four corners of an ROC curve:\n",
    "\n",
    "* **(0, 0):** Represents the strategy of never issuing a positive classification\n",
    "* **(0, 1):** Represents **perfect classification**, i.e. everything is classified to the class it really belongs! \n",
    "* **(1, 0):** We somehow managed to get _everything_ wrong!?\n",
    "* **(1, 1):** Always issue positive classification (example: \"everyone's pregnant!\")\n",
    "\n",
    "The diagonal line between **(0, 0)** and **(1, 1)** represents a randomly guessing \"classifier\" (e.g., flipping a weighted coin).\n",
    "\n",
    "- _Provost, Foster, and Tom Fawcett. Data Science for Business: _\n",
    "  _What you need to know about data mining and data-analytic thinking. O'Reilly Media, Inc., 2013._\n",
    "\n",
    "\n",
    "<img src=\"images/ROC2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC\n",
    "\n",
    "Finally, we can measure the _Area Under the (ROC) Curve_ (ROC AUC), which is the fraction of the total area below our classifier's ROC curve.\n",
    "_Remember:_ Despite its simplicity, you should approach the AUC just like any other evaluation measures: whether it's the right thing to use depends on your business application and domain!\n",
    "\n",
    "Let's go back to our mailing campaign example and compute the AUC there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv(\"data/mailing.csv\")  # Load the data\n",
    "original.head()  # Let's take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original.copy()\n",
    "for field in ['rfaa2', 'pepstrfl']:  # Do the same thing for the two fields of interest\n",
    "    # Go through each possible value \n",
    "    for value in data[field].unique():\n",
    "        # Create a new binary field\n",
    "        data[field + \"_\" + value] = pd.Series(data[field] == value, dtype=int)\n",
    "\n",
    "    # Drop the original field\n",
    "    data = data.drop([field], axis=1)\n",
    "    \n",
    "# Let's look at the data again, after the modifications\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"data\" dataframe contains everything together.\n",
    "# Let's be picky this time w.r.t. the features that we'll use\n",
    "data['date_diff'] = 1 + (data['Lastdate'] - data['Firstdate'])\n",
    "X = data[['Income', 'Amount', 'rfaf2', 'glast', 'gavr', 'date_diff']]\n",
    "Y = data['class']\n",
    "\n",
    "# Keep 75% of the data for training and 25% for testing. We have used this method before.\n",
    "X_mailing_train, X_mailing_test, Y_mailing_train, Y_mailing_test = train_test_split(X, Y, train_size=.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pick a list of regularization values\n",
    "regul_vals = [0.001, 0.01, 1.0]\n",
    "\n",
    "\n",
    "for c in regul_vals:\n",
    "\n",
    "    # Fit a logistic regression model with the respective parameter.\n",
    "    model = LogisticRegression(C=c)\n",
    "    model.fit(X_mailing_train, Y_mailing_train)\n",
    "\n",
    "    # Get the probability of Y_test records being = 1\n",
    "    Y_test_probability_1 = model.predict_proba(X_mailing_test)[:, 1]\n",
    "\n",
    "    # Use the metrics.roc_curve function to get the true positive rate (tpr) and false positive rate (fpr)\n",
    "    # We will not use the thresholds parameter, but it is returned by the method\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(Y_mailing_test, Y_test_probability_1)\n",
    "\n",
    "    # Compute the AUC for the ROC\n",
    "    auc = metrics.roc_auc_score(Y_mailing_test, Y_test_probability_1)\n",
    "\n",
    "    # Plot the ROC curve. Report the AUC in the legend\n",
    "    plt.plot(fpr, tpr, label=\"AUC (C=%.3f) = %.2f\" % (c, round(auc, 2)))\n",
    "    \n",
    "plt.xlabel(\"False positive rate (fpr)\")\n",
    "plt.ylabel(\"True positive rate (tpr)\")\n",
    "plt.plot([0,1], [0,1], 'k--', label=\"Random\")\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Lift Curve\n",
    "\n",
    "The ROC Curve has been a fundamental tool in evaluating the performance of classifiers, for a number of reasons. (can you think of any?).\n",
    "\n",
    "A potential downside is that **FPR** and **TPR** may be tricky quantities to express / explain in a business setting. The CRC is perhaps more \"approachable\".\n",
    "\n",
    "Another type of curve that we often user is known as the **Lift Curve**. It tells us _how much better_ is one model with respect to another. To compute our Lift Curve, we will start with our CRC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's switch gears again and see what happens if we use all the features\n",
    "X = data.drop(['class'], axis=1)\n",
    "\n",
    "# Keep 75% of the data for training and 25% for testing. We have used this method before.\n",
    "X_mailing_train, X_mailing_test, Y_mailing_train, Y_mailing_test = train_test_split(X, Y, train_size=.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_compute_crc( model, x_train, y_train, x_test, y_test):\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Let's get the probabilities once more\n",
    "    probabilities = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    ## We practically ask for descending order of the probabilities\n",
    "    order = np.argsort(probabilities)[::-1]\n",
    "\n",
    "    actual_class_sorted = np.array(y_test)[order]  # \n",
    "    return np.cumsum(actual_class_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a few classifiers here and try to plot everything together.\n",
    "model = LogisticRegression(C=10)\n",
    "logReg10_crc = train_and_compute_crc( model, X_mailing_train, Y_mailing_train, X_mailing_test, Y_mailing_test )\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=15)\n",
    "dec_tree_crc = train_and_compute_crc( model, X_mailing_train, Y_mailing_train, X_mailing_test, Y_mailing_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the x-axis compute the fraction of instances we have seen so far\n",
    "x_lift = np.cumsum( np.ones(len(X_mailing_test)) )\n",
    "x_lift = x_lift / len(X_mailing_test)\n",
    "\n",
    "\n",
    "# Compute the LIFT for the decision tree.\n",
    "# a) Normalize the CRC by the maximum value\n",
    "# b) Divide each value by the fraction of items we have seen so far\n",
    "dec_tree_lift = dec_tree_crc / np.max(dec_tree_crc)\n",
    "dec_tree_lift = dec_tree_lift / x_lift\n",
    "\n",
    "\n",
    "# Compute the LIFT for the Logistic Regression.\n",
    "# a) Normalize the CRC by the maximum value\n",
    "# b) Divide each value by the fraction of items we have seen so far\n",
    "log_reg_lift = logReg10_crc / np.max(logReg10_crc)\n",
    "log_reg_lift = log_reg_lift / x_lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's plot the information\n",
    "\n",
    "plt.figure(figsize=(10,7))  # Create a big enough plot\n",
    "\n",
    "plt.plot(x_lift * 100, dec_tree_lift, label=\"Decision Tree\")  # Show the LIFT curve for the Decision Tree\n",
    "\n",
    "plt.plot(x_lift * 100, log_reg_lift, label=\"Logistic Regression\")  # Show the LIFT curve for Logistic Regression\n",
    "\n",
    "plt.plot([0,100], [1,1], 'k--', label=\"Random\")\n",
    "plt.xlabel(\"Percentage of test instances (decreasing score)\")\n",
    "plt.ylabel(\"Lift (times)\")\n",
    "plt.title(\"Lift curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let's plot the information\n",
    "\n",
    "plt.figure(figsize=(10,7))  # Create a big enough plot\n",
    "\n",
    "plt.plot(x_lift * 100, dec_tree_lift, label=\"Decision Tree\")  # Show the LIFT curve for the Decision Tree\n",
    "\n",
    "plt.plot(x_lift * 100, log_reg_lift, label=\"Logistic Regression\")  # Show the LIFT curve for Logistic Regression\n",
    "\n",
    "plt.plot([0,100], [1,1], 'k--', label=\"Random\")\n",
    "plt.xlim(-0.3, 5)\n",
    "plt.xlabel(\"Percentage of test instances (decreasing score)\")\n",
    "plt.ylabel(\"Lift (times)\")\n",
    "plt.title(\"Lift curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What can you tell me by looking at the above graph?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profit Curves\n",
    "\n",
    "Finally, you should always remember that there is _evaluating the model_ and _evaluating the business_. The two may be linked in one way or another, but this connection has probably not been clear so far. So let's make this happen!\n",
    "\n",
    "Let's generate a simple matrix like the one below.\n",
    "**BE CAREFUL**: The matrix has the structure of a confusion matrix, _but it is not one!_\n",
    "\n",
    "**Question:** Can you see why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_matrix = pd.DataFrame([[17, -1], [0, 0]], columns=['(True) p', '(True) n'], index=['[Predicted] Y', '[Predicted] N'])\n",
    "cost_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above matrix is a _profit matrix_. It says what we _gain_ by targeting a customer that the classifier said that we should target (predicted = \"Y\").\n",
    "\n",
    "We will basically combine our profit matrix with _each_ of our confusion matrices, i.e., for each threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=15)\n",
    "model.fit(X_mailing_train, Y_mailing_train)\n",
    "model_probas = model.predict(X_mailing_test) # [:, 1]\n",
    "\n",
    "\n",
    "# Get the false positive rate, true positive rate, and all thresholds\n",
    "fpr, tpr, thresholds = metrics.roc_curve(Y_mailing_test, model_probas)\n",
    "\n",
    "Size_targeted_pop = float(len(Y_mailing_test))\n",
    "\n",
    "# What is the baseline probability of being positive or negative in the data set?\n",
    "p_p = np.sum(Y_mailing_test)/float(len(Y_mailing_test))\n",
    "p_n = 1 - np.sum(Y_mailing_test)/float(len(Y_mailing_test))\n",
    "\n",
    "\n",
    "# How many users are above the current threshold?\n",
    "n_targeted = []\n",
    "for t in thresholds:\n",
    "    n_targeted.append(np.sum(model_probas >= t))\n",
    "\n",
    "\n",
    "# Turn these counts to percentages of users above the threshold\n",
    "n_targeted = np.array(n_targeted)/float(len(Y_mailing_test))\n",
    "\n",
    "# Expected profits:  \n",
    "expected_profits = (cost_matrix['(True) p']['[Predicted] Y']*(tpr*p_p)) + (cost_matrix['(True) n']['[Predicted] Y']*(fpr*p_n))\n",
    "\n",
    "# Plot the profit curve\n",
    "plt.plot(n_targeted, Size_targeted_pop*expected_profits)\n",
    "plt.xlabel(\"Percentage of users targeted\")\n",
    "plt.ylabel(\"Profit\")\n",
    "plt.title(\"Profits\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # Let's compute the confusion matrix first.\n",
    "# # To do that we need the actual classes and the predicted ones\n",
    "# conf_mtx = metrics.confusion_matrix(Y_mailing_test, predictions, labels=[1, 0])\n",
    "\n",
    "# # Let's turn the confusion matrix into a DataFrame, to make it more presentable\n",
    "# conf_mtx_df = pd.DataFrame(conf_mtx.T, columns=[, ], index=[, '[Predicted] N'])\n",
    "# conf_mtx_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

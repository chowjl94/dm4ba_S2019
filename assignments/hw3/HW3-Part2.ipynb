{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Name: _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your NetID: _\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Part B\n",
    "\n",
    "### <div style=\"color: red\">Read Carefully Before Proceeding</div>\n",
    "\n",
    "If you are having issues running this code because of missing libraries, check the material that we've done in class for installation instructions. This code uses what we have already seen, so if you've been able to execute the code of the Notebooks we've seen in class, you will be fine here as well.\n",
    "\n",
    "\n",
    "You need to answer all questions. Make sure that you answer both **technical** (code-related) and **non-technical** (conceptual) parts of this homework. A lot of code is already available for you, and you can build on that. You are free to use code from our notebooks in class.  All visualizations must be generated by your code, programmatically.\n",
    "\n",
    "\n",
    "Once you're done, download the notebook via `File` -> `Download as` -> `Notebook`, which will fetch a file with an \".ipynb\" extension. Include this file in your submission, as a separate document -- **not** in the word / pdf submission itself. In case you use additional code stored in another directory, make sure to submit that as well.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Mailing Campaign Revisited\n",
    "\n",
    "This homework (mostly) expands on the problem that we've been working with in class: a mailing campaign. The main objective is for you to get a better grasp of a predictive model's performance.\n",
    "\n",
    "In the `data` directory of this homework, you will find two files: `mail_train.tsv` and `mail_test.tsv`.\n",
    "\n",
    "**For now**, use only the `mail_train.tsv` file.\n",
    "\n",
    "For your convenience, both files have **already** been dummysized.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we usually do, start by loading the libraries of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the files**\n",
    "\n",
    "Let's start by reading our training data: the `mail_train.tsv` file.  Again, for now, you can / should omit the test data file entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read the file\n",
    "train_df = pd.read_csv(\"data/mail_train.tsv\",  sep='\\t')\n",
    "\n",
    "# Let's print the top-5 rows\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4.2 - Model Training and Complexity Control\n",
    "\n",
    "Let's go back to our mailing campaign.  As part of your work here, you will need to pick 3 classifiers:\n",
    "\n",
    "* Logistic Regression \n",
    "* Decision Trees\n",
    "* A third classifier of your choice.  (k-NN, Naive Bayes, SVMs, etc)\n",
    "\n",
    "\n",
    "For each of your models, you need to do complexity control, to make sure that it is not overfitting. As we have discussed in the class and seen during the lectures, we can use **cross validation** (on the _training dataset_) to achieve this goal. Therefore, for each model:\n",
    "\n",
    "* You must pick a **complexity parameter** (e.g., $k$, $\\lambda$, depth, etc)\n",
    "* You need to select a **meaningful range of values** for the complexity parameter to test\n",
    "* For each compexity parameter, evaluate your model's performance using **10-fold cross validation**.\n",
    "* You must **collect and analyze** the results\n",
    "* You must pick the **right complexity parameter value** following your analysis. You do _not_ have to create fitting graphs, unless you want to, but your work needs to make it clear **how** you came to the conclusion that this was the right parameter. That is, you need to support your reasoning with code and / or plots, or similar technique.\n",
    "\n",
    "**Suggestion:** Do the above for **1** model first (e.g., Logistic Regression). Once you've done that, you can copy-paste your code for the other 2 models, making sure that you choose the proper evaluation parameter each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 - Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complexity Parameter Selection**\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 - Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complexity Parameter Selection**\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 - Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complexity Parameter Selection**\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4.3 - Best Model: Winner takes all\n",
    "\n",
    "Now that you have tuned the hyper-parameters of each model, you will compare them against each other. In particular, you will split the training dataset into 75% (sub)training and 25% for testing. With the **same** 75% of your (subtraining) data, you will train the 3 different models, with their best performing hyper-parameters - the ones you chose above.\n",
    "\n",
    "You will then **test** the three models against the remaining **25%**. You will then **plot the ROC curve** for each model **on the same graph**. Make sure you have the proper legends in place and that they are visible.\n",
    "\n",
    "The classifier (model) with the **best Area Under the ROC Curve (AUC)** on the 25% will be the winner.\n",
    "\n",
    "\n",
    "When you split the data, you **must** specify the `random_state` parameter and set it to the numerical part of your NetID. \n",
    "\n",
    "**Remember:** For the ROC (and the AUC, subsequently), we want to have a _ranking_ of the predicted values. The ranking is obtained by predicting the _probability_ that an instance belongs to the class of interest (the \"positive\" class: user will donate). Our models can generate those probabilities right away; check our previous notebooks for which method to use.\n",
    "\n",
    "\n",
    "\n",
    "<p/>\n",
    "<div style=\"color: red\"><b>Requirements:</b></div>\n",
    "\n",
    "\n",
    "* **Split your training set** into 75% training, 25% testing. Use the `train_test_split` method. You **must** set the `random_state` to the numerical part of your NetID.\n",
    "* **Train the 3 models** with the **best parameters** (selected earlier) on the 75% of the data. All models must be trained on the same subset.\n",
    "* With each model, **compute the probability of the remaining 25%** to belong to the \"positive\" class. The \"positive\" class is that an individual will donate.\n",
    "* Using those probabilities and the true outcomes, generate the necessary **information to plot the ROC curve**. ( `roc_curve()` )\n",
    "* Using those probabilities and the true outcomes, also **compute the AUC**, for each of the 3 cases (`roc_auc_score()`).\n",
    "* **Plot the three ROCs** together on the same graph for comparison.\n",
    "* Pick the model that exhibits the **highest AUC** (based on the earlier computations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Do not forget to set the random_state to the numerical part of your NetID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4.4 - Which is the best performing model?\n",
    "\n",
    "_Your answer here_\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4.6 - Target Audience Size\n",
    "\n",
    "Imagine that you have a budget of \\\\$600 and each invitation costs \\\\$1.5. What is the maximum number of individuals can we target?\n",
    "\n",
    "**Note:** This question is **independent of** a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here_\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4.7 - Best-case Scenario\n",
    "\n",
    "Assume for a moment that our classifiers can **perfectly** separate between the two classes. Also, each individual will donate (on average) \\\\$5.\n",
    "\n",
    "* In this best-case scenario, how much do we expect to gather in donations?\n",
    "\n",
    "* What will our profit be, after we account for the cost of sending the invites)?\n",
    "\n",
    "Note that we are still bound by the budgetary constraint mentioned earlier.\n",
    "\n",
    "\n",
    "**Note:** This question is **independent of** a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here_\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4.8 - Generate a Profit Curve\n",
    "\n",
    "Assume that each invitation costs **\\$1.5**, and the average individual contribution (out of those who contribute) is **\\$5**. Also, our budget is **\\$600**.\n",
    "\n",
    "To properly evaluate our approach for this type of problem, we need to use profit curves. Here, you'll generate a profit curve using your best performing classifier, which you picked earlier (based on the AUC measure).\n",
    "\n",
    "With that, you will **train your classifier on the 75% split** of the training data and you will **generate a Profit Curve** by applying it on the remaining 25% of the (training) data. In case you use the `train_test_split` method again, you **must** specify the `random_state` parameter and set it to the numerical part of your NetID.\n",
    "\n",
    "Recall that for a profit curve, you need to predict the probability of an instance to be in the \"will donate\" (positive) class. Check Module 5 for code to generate a Profit Curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4.9 - Best Threshold\n",
    "\n",
    "Using your earlier code and results, answer the following questions.\n",
    "<div style=\"color: red\">Your answers must be answered using code, not approximate estimations from the graph above.</div>\n",
    "\n",
    "\n",
    "\n",
    "**1.** For which threshold does your model hit its maximum?\n",
    "\n",
    "**2.** How many people should be invited (according to the model) to hit the maximum profit?\n",
    "\n",
    "**3.** Can you invite that many people? Why / Why not ?\n",
    "\n",
    "**4.** What will be your _actual_ profit, based on how many people you will finally invite? What is your _actual_ threshold?\n",
    "\n",
    "\n",
    "\n",
    "**Hint:** Let's say that you have stored the expected profits in a variable called `expected_profits`, as was done in the notebook of Module 5. To help you answer this question, you need to find the position in the `expected_profits` that the value gets maximum. You can easily find that position by doing: `np.argmax(expected_profits)`. \n",
    "\n",
    "**Attention:** That method does _not_ tell you what the maximum is. It does not say what the threshold is either.\n",
    "It will only give you the _index_ in the `expected_profits` array that has the max value. See the code below for an example.\n",
    "\n",
    "We are passing an entire list as an argument and we ask: what is the index in the list that has the maximum element ? Recall that _indexing_ in Python starts from 0, i.e., the 1st element is at index 0, the 2nd element is at index 1, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple list\n",
    "l = [ 2, 1, 5, 8, -1 ]\n",
    "np.argmax( l )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above says that the maximum element is at index 3. If we then do `l[3]`, the result we will get will be 8 (the actual max value)!\n",
    "\n",
    "Use the `np.argmax( expected_profits )` to find where the expected profits becomes maximum for your classifier. Using that index result, you can then \"probe\" any other arrays that you are using to store the thresholds or the number of people who are targeted (i.e., invited) and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4.9 - Deployment time\n",
    "\n",
    "It is time to send out invitations. To do so, you will rely on the results of your best performing classifier.\n",
    "\n",
    "- Train your _best performing_ model on the entire training dataset. \n",
    "\n",
    "### <div style=\"color: red\">DO NOT TRAIN THE MODEL ON THE TESTING DATA</div>\n",
    "\n",
    "- Apply your model on the testing dataset and get the probability, for each test instance, to donate.\n",
    "\n",
    "- You will invite the individuals based on the earlier results. You may only invite people who have a probability greater than the _threshold_ that you picked earlier. Remember that you also have a _cap_ on the number of people who can be invited.\n",
    "\n",
    "Following the above guidelines - and restrictions -, implement your solution so that you can answer the following questions;\n",
    "\n",
    "1. How many people will you invite?\n",
    "1. How much profit will you make? \n",
    "1. Given your budget and your profit, does / did it make sense to target those individuals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read the test file\n",
    "test_df = pd.read_csv(\"data/mail_test.tsv\",  sep='\\t')\n",
    "\n",
    "# Let's print the top-5 rows\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

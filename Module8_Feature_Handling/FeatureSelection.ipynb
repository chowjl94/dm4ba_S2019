{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining for Business Analytics\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "Spring 2019 - Prof. George Valkanas\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step in addressing a problem is coming up with a comprehensive set of features that will be useful for our problem. This is what we often refer to as **feature engineering**. \n",
    "\n",
    "However, while working towards our goal, we may actually generate too many features, or features that work \"against\" our goal -- and there may be a few reasons for that. In situations like that, we would like to perform **feature selection**, i.e., pick a subset of the original features that we have, which would yield a better performance in the end.\n",
    "\n",
    "We may also do feature selection so that our model is more _explainable_, because it will contain fewer features to describe / work with.\n",
    "\n",
    "Below, we are going to discuss a few techniques that approach this problem.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read the file\n",
    "train_df = pd.read_csv(\"data/mail_train.tsv\",  sep='\\t')\n",
    "\n",
    "# Let's print the top-5 rows\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats = train_df.drop('TARGET_B', axis=1)\n",
    "train_labels = train_df['TARGET_B']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models with Inherent Feature Selection\n",
    "\n",
    "Certain models perform feature selection by themselves, because of how they work.\n",
    "\n",
    "**Question:** Can you think of a model that we've seen that does that?\n",
    "\n",
    "As we've discussed this model extensively in the past, we will not consider it here again.\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection with $L1$-Regularization\n",
    "\n",
    "This approach isn't always applicable due to how it works and its relevance to regularization. However, it is an interesting and useful thing to know about.\n",
    "\n",
    "**L1** regularization performs _feature selection_ ! \n",
    "\n",
    "Let's see this in practice with our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regul_df = pd.DataFrame()\n",
    "\n",
    "for c in [100, 10, 1, 0.1, 0.01, 0.001]:\n",
    "    model = LogisticRegression(C=c, penalty='l1', max_iter=1000000, solver='liblinear' )\n",
    "    model.fit( train_feats, train_labels )\n",
    "\n",
    "    regul_df[c] = model.coef_[0]\n",
    "\n",
    "# Let's see what the dataframe looks like\n",
    "regul_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-Class\n",
    "\n",
    "Plot the dataframe above. It will be useful to plot them as `subplots` and use a `bar` kind of plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Feature Selection\n",
    "\n",
    "A popular technique, which has received more interest recently due to access to greater computational resources, is a process called **Forward Feature Selection**.\n",
    "\n",
    "The process works in iterations, each time picking the single best feature which will improve the performance of a model. The performance measure is selected according to our objective (e.g., precision, AUC, accuracy, etc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_next_subset( all_feats, labels, known_good_feats ):\n",
    "\n",
    "    best_score = 0\n",
    "    best_subset = None\n",
    "    for f in all_feats.columns.tolist():\n",
    "\n",
    "        # If we've selected this feature already, do not consider it again\n",
    "        if f in known_good_feats:\n",
    "            continue\n",
    "\n",
    "        # Create a copy of the good features, so that we can append the new one\n",
    "        # This will be the feature subset for this iteration\n",
    "        iter_feat_subset = list(known_good_feats)\n",
    "        iter_feat_subset.append(f)\n",
    "\n",
    "        # From the dataframe, get the columns of interest\n",
    "        train_subset = all_feats[iter_feat_subset]\n",
    "\n",
    "        # For this example we are training a Logistic Regression Classifier\n",
    "        # model = LogisticRegression(solver='lbfgs', )\n",
    "        model = MultinomialNB()\n",
    "        avg_score = np.mean( cross_val_score( model, train_subset, labels, scoring='roc_auc', cv=3 ) )\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_subset = iter_feat_subset\n",
    "\n",
    "    return best_subset, best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### In-Class\n",
    "\n",
    "Write code below that will use the method above and will help you pick the subset with the best 7 performing features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction Techniques\n",
    "\n",
    "Another popular approach for this type of problem is _dimensionality reduction_. To briefly understand PCA consider the following dataset:\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/jPw90.png\" width=\"50%\" />\n",
    "\n",
    "PCA will try to find a \"line\" (in this case) that \"best\" describes the data points. There are several such lines as we can see from the dataset below: <img src=\"https://i.stack.imgur.com/Q7HIP.gif\" width=\"50%\" />\n",
    "\n",
    "And here's what the solution of this example will look like:\n",
    "<img src=\"https://i.stack.imgur.com/lNHqt.gif\" width=\"50%\" />\n",
    "\n",
    "\n",
    "Images from [this excellent answer](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues) on stack exchange regarding PCA.\n",
    "\n",
    "The `sklearn` package approaches and works with PCA (and a similar technique, known as SVD) in the same way that we used the `CountVectorizer` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example training PCA with 10 components (expected dimensions)\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(train_feats)\n",
    "\n",
    "# Just like with count vectorizer!\n",
    "train_pca_feats = pca.transform(train_feats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-Class / Take Home\n",
    "\n",
    "Generate different PCA transformations, up to 7 components / dimensions, so that we can compare them with the previous result (forward feature selection). Use a `LogisticRegression` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-Class / Take Home\n",
    "\n",
    "Plot the various performances together on the same figure to see which one performs the best.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining for Business Analytics\n",
    "\n",
    "## Similarity, Neighbors\n",
    "\n",
    "Spring 2019 - Prof. George Valkanas\n",
    "\n",
    "Material based on content courtesy of Prof. Foster Provost\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "We already saw how we can use _similarity_ between two instances to produce recommendations, even in the absence of a target variable.\n",
    "\n",
    "_Similarity_ is also a key element in **clustering**, i.e., the generation of _natural groups_ of our instances. We have already seen that different similarity measures result in different _rankings_ of the same set of instances. With that in mind, the _similarity_ that we use greatly affects the result of our **clustering** algorithms.\n",
    "\n",
    "\n",
    "Below, we discuss 2 different clustering techniques:\n",
    "* $k$-Means\n",
    "* Hierarchical Clustering\n",
    "\n",
    "Through our discussion, you need to be able to understand:\n",
    "1. How **clustering** differs from **classification**\n",
    "1. What is _the result_ of a clustering algorithm\n",
    "1. How $k$-Means works\n",
    "1. How hierarchical clustering works\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the following image\n",
    "\n",
    "<img src=\"images/raw_data.png\" />\n",
    "\n",
    "\n",
    "How many _clusters_ / \"natural groupings\" of the data do you see?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the same image, where the data points are associated with one of two classes: Red or Blue.\n",
    "\n",
    "<img src=\"images/class_data.png\" />\n",
    "\n",
    "During _classification_ , our data points (instances) come with those _predefined_ classes. The class value of an instance appears in the _target variable_.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Using the two earlier images and the brief explanations above, how do _you_ think that clustering differs from classification?\n",
    "\n",
    "\n",
    "**Answer:** (write down your thoughts)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial import distance\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.figsize'] = 10, 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans\n",
    "\n",
    "The \"classic\" approach for findings clusters is via the **$k$-Means algorithm**, which will find a set of $k$ clusters.\n",
    "\n",
    "The value $k$ is a **parameter** to the model, i.e. **we** must provide the number of clusters that we expect the algorithm to find.\n",
    "\n",
    "**Question:** _What_ is a good $k$ value for the algorithm?\n",
    "\n",
    "\n",
    "Here's a video of how the algorithm works: https://www.youtube.com/watch?v=BVFG7fd1H30\n",
    "\n",
    "\n",
    "The $k$-Means algorithm is implemented in all major (self-respecting) data mining libraries. It's also available under **sklearn.cluster**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read our Whiskey data again\n",
    "data = pd.read_csv(\"data/scotch.csv\")\n",
    "data = data.drop([u'age', u'dist', u'score', u'percent', u'region', u'district', u'islay', u'midland', u'spey', u'east', u'west', u'north ', u'lowland', u'campbell', u'islands'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a kMeans model over the whiskeys. We want 6 clusters.\n",
    "k_clusters = 6\n",
    "\n",
    "## Fit clusters like in our previous models/transformations/standarization \n",
    "## (e.g. Logistic, Vectorization,...)\n",
    "\n",
    "model = KMeans(k_clusters)\n",
    "model.fit(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "What clusters do we get? Let's get \"predictions\" of the model. To do that, we will use the `predict()` method of the kmeans cluster.\n",
    "\n",
    "**Question:** Clustering is an **unsupervised** task. What do you think we mean by \"prediction\" here then?\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "**Question:** How do we compute this distance between an instance and a cluster?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Records in our dataset (rows): \", len(data.index))\n",
    "print (\"Then we predict one cluster per record, which means length of: \", len(model.predict(data)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The index index contains the names of the whiskeys that we know of\n",
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = model.predict(data)\n",
    "\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think that the above numbers mean?\n",
    "***\n",
    "\n",
    "Let's print each instance with its associated cluster **and** the cluster _centroids_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "data_predicts = model.predict(data)\n",
    "for i, cid in zip(data.index, data_predicts):\n",
    "    row_list = [i, cid]\n",
    "    row_list.extend(model.cluster_centers_[cid])\n",
    "    l.append(row_list)\n",
    "\n",
    "colNames = ['Whiskey','Cluster_predicted']\n",
    "colNames.extend( [ 'Cluster ' + c for c in data.columns ]  )\n",
    "pd.DataFrame( l, columns=colNames )[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put each cluster into its own column!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_listing = {}\n",
    "for cluster in range(k_clusters):\n",
    "    cluster_name = 'Cluster ' + str(cluster)\n",
    "    cluster_listing[cluster_name] = [''] * len(data)\n",
    "    where_in_cluster = np.where(clusters == cluster)[0]\n",
    "    cluster_listing[cluster_name][0:len(where_in_cluster)] = data.index[where_in_cluster]\n",
    "\n",
    "# Print clusters\n",
    "pd.DataFrame(cluster_listing).loc[0:np.max(np.bincount(clusters)) - 1,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think, to the results make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another clustering application, closer to home (all puns intended)\n",
    "\n",
    "Well, we can cluster whiskeys, we can cluster wines and we can cluster documents (e.g., news articles), but whether the results are particularly meaningful or not is often open for interpretation and left as follow-up work for the data scientist, in collaboration with a domain expert (i.e., a wine taster perhaps?).\n",
    "\n",
    "So, let's work with some data that we can get a better understanding of: _locations_ ! \n",
    "\n",
    "\n",
    "Under the `data/` directory, there is a file with GPS coordinates in it. The file contains several lines, each of which is a GPS signal (longitude, latitude). Let's read the file and see what it's about.  The file uses the comma `','` as a separator between the columns, so we'll specify that. The data is also in decimal format (`float`).\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = pd.read_csv( \"data/locations.csv\", sep=',', dtype=float )\n",
    "coordinates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of me telling you what these locations are, let's actually show them on a figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 7))  # Small plot at first\n",
    "plt.scatter( coordinates['long'], coordinates['lat'], c='black', s=(72./fig.dpi)**2 )  # The value next to 's' is the size of a pixel!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this blob!? We can't make anything out of it! OK, it seems that most of the information is centered around some specific ranges though.\n",
    "\n",
    "So, let's plot again by focusing on these ranges and we'll blow up the image a bit more. For the ranges, we'll use the `xlim()` and `ylim()` methods of the matplotlib library.\n",
    "\n",
    "Try it in the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))  # Let's create a bigger picture\n",
    "\n",
    "plt.scatter( coordinates['long'], coordinates['lat'], c='black', s=(72./fig.dpi)**2 )  # The value next to 's' is the size of a pixel!\n",
    "\n",
    "# Pick the ranges that will help you figure out what the shape is about\n",
    "plt.xlim( None, None )  # That's for the x-coordinate range. Format is: x_min, x_max\n",
    "plt.ylim( None, None )  # That's for the y-coordinate range. Format is: x_min, x_max\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! Now it starts to make sense. OK, let's use these coordinates to actually find some clusters of the coordinates.\n",
    "\n",
    "Now, $k$-Means is fast, but it's not \"classroom demo\" fast. For that reason, we'll _sample_ our coordinates and use only 10000 of them. You can try with the entire dataset on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_coords = coordinates.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create 30 clusters. You can try different values and see how the results change.\n",
    "k_clusters = 30\n",
    "\n",
    "model = KMeans(k_clusters)\n",
    "\n",
    "# Train the model on the sample coordinates, not the original.\n",
    "model.fit(sample_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict the cluster that each coordinate belongs to!\n",
    "# Notice that we are now doing this for ALL coordinates\n",
    "predict_ids = model.predict(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the matrix: get every row as a column and every column as a row\n",
    "# We do this so that we can pass the information to the scatter plot below.\n",
    "clust_coords = model.cluster_centers_.T\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "plt.scatter( coordinates['long'], coordinates['lat'], c=predict_ids, s=(72./fig.dpi)**2, cmap='tab20' )\n",
    "plt.scatter( clust_coords[0], clust_coords[1], c='black', s=50, marker='*' )\n",
    "\n",
    "# Use the coordinates that you had earlier, to make the plot look nice!\n",
    "plt.xlim( None, None )\n",
    "plt.ylim( None, None )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Do the star-shaped points make _some_ sense? Do the colored regions make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div style=\"color: red\">A note on visualization</div>\n",
    "\n",
    "The above exercise should show the importance of _visualizing_ information for several reasons.\n",
    "\n",
    "- First, it is more intuitive and easier to explain -- an image is said to be worth 1000 words after all! There is, in fact, an entire science / discipline around visualizing information.\n",
    "\n",
    "- Second, this type of visualization allows us to see issues that we have with our data. Can you spot any?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "kMeans is just one way to find similar groups, but not the only one. Another very common method is Hierarchical Clustering.\n",
    "\n",
    "First let's look at a simple example to illustrate.  Given a set of records (A-F) with two features, we can visualize them on a 2 dimensional surface.  Clustering proceeds as follows.  First consider each point to be its own cluster.  Then, iteratively, group together the closest two clusters.  In the figure, circles were drawn in order of grouping.  The second diagram is a visualization of the hierarchy of groupings, called a \"dendrogram.\"  You can clip it at any point, vertically, and get \"the best\" clustering for a certain number of groups.\n",
    "\n",
    "\n",
    "<img src=\"images/cutting.png\" height=40% width=40%>\n",
    "\n",
    "Here is a visualization of a part of the dendrogram for the whiskey clustering in the book:\n",
    "\n",
    "<img src=\"images/cross_section.png\" height=70% width=70%>\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrograms in code\n",
    "\n",
    "We'll go back to our Whiskey example to illustrate the dendrogram. We will use the module **scipy.cluster.hierarchy** for this purpose.\n",
    "\n",
    "The approach is as follows:\n",
    "1. We start by computing the distance between every pair of whiskeys\n",
    "1. Then, we will use the `linkage()` method coming from the `scipy.cluster.hierarchy` module. This method will actually perform the hierarchical clustering we want! Notice the difference between this approach and creating the k-Means model. More information about the method is available [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html)\n",
    "1. We then pass the resulting hierarchical clustering to the `dendrogram()` method, which will visualize the information in the way that we described earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets pairwise distances between observations in n-dimensional space.\n",
    "dists = distance.pdist(data, metric=\"cosine\")\n",
    "\n",
    "# This scipy's function performs hierarchical/agglomerative clustering on the condensed distance matrix y.\n",
    "links = linkage(dists, method='average')\n",
    "\n",
    "# Now we want to plot those 'links' using \"dendrogram\" function\n",
    "# ATTENTION: This line must come BEFORE the dendrogram() method below!!!\n",
    "plt.rcParams['figure.figsize'] = 32, 16\n",
    "\n",
    "den = dendrogram(links)\n",
    "\n",
    "plt.xlabel('Samples',fontsize=18)\n",
    "plt.ylabel('Distance',fontsize=18)\n",
    "plt.xticks(rotation=90,fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use other measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets pairwise distances between observations in n-dimensional space.\n",
    "dists = distance.pdist(data, metric=\"euclidean\")\n",
    "\n",
    "# This scipy's function performs hierarchical/agglomerative clustering on the condensed distance matrix y.\n",
    "links = linkage(dists, method='average')\n",
    "\n",
    "# Now we want to plot those 'links' using \"dendrogram\" function\n",
    "plt.rcParams['figure.figsize'] = 32, 16\n",
    "\n",
    "den = dendrogram(links)\n",
    "\n",
    "plt.xlabel('Samples',fontsize=18)\n",
    "plt.ylabel('Distance',fontsize=18)\n",
    "plt.xticks(rotation=90,fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "Clusters **do not** come with predetermined labels. Can you think of an approach to \"characterize them\" ? Can you think of an _automated approach_ to characterize them?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "Both $k$-Means and Hierarchical Clustering, as described earlier perform **hard clustering**. What do you think that means?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Text\n",
    "\n",
    "#### (Very Briefly)\n",
    "\n",
    "Clustering documents (e.g., articles) is a very common task, as it allows us to (very) quickly understand what the corpus (i.e., the entire set of documents) is about. In other words, the task is to figure out the various **topics** discussed in the corpus.\n",
    "\n",
    "For this type of problem, instead of $k$-Means, we typically use another technique, called  **Latent Dirichlet allocation** (LDA). LDA works differently from $k$-Means, but in the end we still obtain a set of \"clusters\" / topics, which are characterized by words that appear in the document. \n",
    "\n",
    "Just like $k$-Means, LDA expects a parameter $k$ for the number of topics to discover.\n",
    "\n",
    "Unlike $k$-Means, LDA does _soft_ clustering. What do you think this means?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

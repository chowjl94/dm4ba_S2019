{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining for Business Analytics\n",
    "\n",
    "## Similarity, Neighbors\n",
    "\n",
    "Spring 2019 - Prof. George Valkanas\n",
    "\n",
    "Material based on content courtesy of Prof. Foster Provost\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "We already saw how we can use _similarity_ between two instances to produce recommendations, even in the absence of a target variable.\n",
    "\n",
    "_Similarity_ is also a key element in **clustering**, i.e., the generation of _natural groups_ of our instances. We have already seen that different similarity measures result in different _rankings_ of the same set of instances. With that in mind, the _similarity_ that we use greatly affects the result of our **clustering** algorithms.\n",
    "\n",
    "\n",
    "Below, we discuss 2 different clustering techniques:\n",
    "* $k$-Means\n",
    "* Hierarchical Clustering\n",
    "\n",
    "Through our discussion, you need to be able to understand:\n",
    "1. How **clustering** differs from **classification**\n",
    "1. What is _the result_ of a clustering algorithm\n",
    "1. How $k$-Means works\n",
    "1. How hierarchical clustering works\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import math\n",
    "import pyproj as proj\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.figsize'] = 10, 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans\n",
    "\n",
    "The \"classic\" approach for findings clusters is via the **$k$-Means algorithm**, which will find a set of $k$ clusters.\n",
    "\n",
    "The value $k$ is a **parameter** to the model, i.e. **we** must provide the number of clusters that we expect the algorithm to find.\n",
    "\n",
    "**Question:** _What_ is a good $k$ value for the algorithm?\n",
    "\n",
    "\n",
    "Here's a video of how the algorithm works: https://www.youtube.com/watch?v=BVFG7fd1H30\n",
    "\n",
    "\n",
    "The $k$-Means algorithm is implemented in all major (self-respecting) data mining libraries. It's also available under **sklearn.cluster**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read our Whiskey data again\n",
    "data = pd.read_csv(\"data/scotch.csv\")\n",
    "data = data.drop([u'age', u'dist', u'score', u'percent', u'region', u'district', u'islay', u'midland', u'spey', u'east', u'west', u'north ', u'lowland', u'campbell', u'islands'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_clusters = 6\n",
    "\n",
    "## Fit clusters like in our previous models/transformations/standarization \n",
    "## (e.g. Logistic, Vectorization,...)\n",
    "\n",
    "model = KMeans(k_clusters)\n",
    "model.fit(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What clusters do we get? Let's get \"predictions\" of the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Records in our dataset (rows): \", len(data.index))\n",
    "print (\"Then we predict one cluster per record, which means length of: \", len(model.predict(data)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = model.predict(data)\n",
    "\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(zip(data.index,model.predict(data))), columns=['Whiskey','Cluster_predicted']) [0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put each cluster into its own column!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_listing = {}\n",
    "for cluster in range(k_clusters):\n",
    "    cluster_listing['Cluster ' + str(cluster)] = [''] * 109\n",
    "    where_in_cluster = np.where(clusters == cluster)[0]\n",
    "    cluster_listing['Cluster ' + str(cluster)][0:len(where_in_cluster)] = data.index[where_in_cluster]\n",
    "\n",
    "# Print clusters\n",
    "pd.DataFrame(cluster_listing).loc[0:np.max(np.bincount(clusters)) - 1,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "There are different ways to find similar groups.  One very common method is Hierarchical Clustering.\n",
    "\n",
    "First let's look at a simple example to illustrate.  Given a set of records (A-F) with two features, we can visualize them on a 2 dimensional surface.  Clustering proceeds as follows.  First consider each point to be its own cluster.  Then, iteratively, group together the closest two clusters.  In the figure, circles were drawn in order of grouping.  The second diagram is a visualization of the hierarchy of groupings, called a \"dendrogram.\"  You can clip it at any point, vertically, and get \"the best\" clustering for a certain number of groups.\n",
    "\n",
    "\n",
    "<img src=\"images/cutting.png\" height=40% width=40%>\n",
    "\n",
    "Here is a visualization of a part of the dendrogram for the whiskey clustering in the book:\n",
    "\n",
    "<img src=\"images/cross_section.png\" height=70% width=70%>\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's examine the dendrogram(s) for our data, we'll be using the library: **scipy.cluster.hierarchy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets pairwise distances between observations in n-dimensional space.\n",
    "dists = pdist(data, metric=\"cosine\")\n",
    "\n",
    "# This scipy's function performs hierarchical/agglomerative clustering on the condensed distance matrix y.\n",
    "links = linkage(dists, method='average')\n",
    "\n",
    "# Now we want to plot those 'links' using \"dendrogram\" function\n",
    "plt.rcParams['figure.figsize'] = 32, 16\n",
    "\n",
    "den = dendrogram(links)\n",
    "\n",
    "plt.xlabel('Samples',fontsize=18)\n",
    "plt.ylabel('Distance',fontsize=18)\n",
    "plt.xticks(rotation=90,fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use other measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets pairwise distances between observations in n-dimensional space.\n",
    "dists = pdist(data, metric=\"euclidean\")\n",
    "\n",
    "# This scipy's function performs hierarchical/agglomerative clustering on the condensed distance matrix y.\n",
    "links = linkage(dists, method='average')\n",
    "\n",
    "# Now we want to plot those 'links' using \"dendrogram\" function\n",
    "plt.rcParams['figure.figsize'] = 32, 16\n",
    "\n",
    "den = dendrogram(links)\n",
    "\n",
    "plt.xlabel('Samples',fontsize=18)\n",
    "plt.ylabel('Distance',fontsize=18)\n",
    "plt.xticks(rotation=90,fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "Clusters **do not** come with predetermined labels. Can you think of an approach to \"characterize them\" ? Can you think of an _automated approach_ to characterize them?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

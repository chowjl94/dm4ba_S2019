{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining for Business Analytics\n",
    "\n",
    "## Similarity, Neighbors\n",
    "\n",
    "Spring 2019 - Prof. George Valkanas\n",
    "\n",
    "Material based on content courtesy of Prof. Foster Provost\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial import distance\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.figsize'] = 10, 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Wine Recommendation\n",
    "\n",
    "**Customer:** I'd like to order the _Cabernet Sauvignon_ \n",
    "\n",
    "**Waiter:** We're just out of this wine.\n",
    "\n",
    "**Customer:** What would be _the closest_ wine that you currently have?\n",
    "\n",
    "**Waiter:** We have some very nice _Merlot_ or you could try the _Malbec-Cabernet_\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### <div style=\"color: red\">How did the waiter know about this recommendation?</div>\n",
    "\n",
    "Did she/he know that the customer _will like_ the suggestion? How would _you_ make such a recommendation ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Plagiarism Detection / Near-Duplicate Detection in Web Search\n",
    "\n",
    "\n",
    "Plagiarism is not just an unethical practice, but can also have legal ramifications. Plagiarism arises when someone (a person, an entity) copies someone else's (literary) work (without the proper attribution) and tries to pass it as their own.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: right\"><i>\"When you steal from one author, it's plagiarism; if you steal from many, it's research.\"<br/> Wilson Mizner</i>\n",
    "</div>\n",
    "\n",
    "\n",
    "Meanwhile, when you search the web with your favorite search engine, you'd like to get back results, each of which conveys _new_ information. That is, if all of the first 10 results **all** present **the same** information, you'd not be very happy about it. The first link will have new information (and answer your question), the second one may just reinforce what you learned from the 1st link, but after that it's just more of the same.\n",
    "\n",
    "\n",
    "Notice that these two problems are effetively the same. The way that the results are used is different in the two scenarios.\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### <div style=\"color: red\">How can we solve this problem?</div>\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Player Substitution\n",
    "\n",
    "Imagine that you're playing your favorite video game (football, basketball, etc) and one of the players gets injured and needs to be taken off the court / field. \n",
    "\n",
    "Which player do you replace them with?\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fundamental Question\n",
    "\n",
    "We have seen various classifiers until now. The main question to answer here is the following:\n",
    "\n",
    "**<div style=\"text-align: center\">Do we have a target variable that we can use?</div>** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Show me your friends..._\n",
    "\n",
    "Unlike magnets, _similars_ attract when it comes to people. That is, our friends' interests are more aligned with our own. Another way to see this is that we may tend to befriend individuals who have _common interests_ with our own.\n",
    "\n",
    "In abstract terms, we are _more similar_ with people we are friends with and _less similar_ with people who we (knowingly / decidedly) aren't friends with.\n",
    "\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are you talking about?\n",
    "\n",
    "Similarity between **people**? Between **wines**? Between **texts**? What are you talking about? How can these things be possibly related?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start simple\n",
    "\n",
    "- Can you determine the similarity between two integers? For example, which two numbers would you say are _more similar_? Numbers 9 and 10,  or the numbers 1 and 100?\n",
    "    * What about negative numbers? How about -3 and -4 compared against -100 and -98?\n",
    "\n",
    "\n",
    "- Can you come up with a similarity measure for two real-valued numbers? Is it any different from integer values?\n",
    "\n",
    "\n",
    "- Can you measure the similarity between GPS signals? A GPS signal is a pair of (x, y) coordinates, commonly refered to as _latitude_ and _longitude_. Let's say that you are given 3 pairs of such GPS coordinates:\n",
    "    * **GPS 1:** (1, 1)\n",
    "    * **GPS 2:** (1, 0.5)\n",
    "    * **GPS 3:** (2, 2)\n",
    "    \n",
    "  Which pair of GPS coordinates is _more similar_ ? How did you compute the similarity?\n",
    "\n",
    "\n",
    "- OK, let's go for something more involved. Imagine that you are given 3 baskets with fruits:\n",
    "    * **Basket 1:** Fruit_1, Fruit_2, Fruit_3\n",
    "    * **Basket 2:** Fruit_1, Fruit_2, Fruit_4, Fruit_5\n",
    "    * **Basket 3:** Fruit_3, Fruit_2\n",
    "    \n",
    "  What if our basket had multiples of the same fruit, e.g., **Bakset 1:** Fruit_1, Fruit_1, Fruit_2, Fruit_3 ?\n",
    "    \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the difficult cases\n",
    "\n",
    "How do you define the similarity between:\n",
    "\n",
    "- Two _wines_?\n",
    "- Two _pieces of text_ ?\n",
    "- Two _people_ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalizing from our previous examples, we are discussing about the similarity of _objects_. \n",
    "\n",
    "**Question:** Did we use just a single similarity?\n",
    "\n",
    "In case it's also not obvious by now, **to compute the similarity between _objects_ we need to have quantifiable _features_**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enough talk! Let's work with some Data\n",
    "\n",
    "Following that discussion, we have compiled a scotch whiskey data set. You can find it in `data/scotch.csv`.\n",
    "\n",
    "The data consists of 5 general whiskey attributes, each of which has many possible values:\n",
    "\n",
    "- **Color**: yellow, very pale, pale, pale gold, gold, old gold, full gold, amber, etc.\n",
    "- **Nose**: aromatic, peaty, sweet, light, fresh, dry, grassy, etc.\n",
    "- **Body**: soft, medium, full, round, smooth, light, firm, oily.\n",
    "- **Palate**: full, dry, sherry, big, fruity, grassy, smoky, salty, etc.\n",
    "- **Finish**: full, dry, warm, light, smooth, clean, fruity, grassy, smoky, etc.\n",
    "\n",
    "Let's read the file in and take a look. For convenience, we have also dummysized the data. There are a few other features unrelated to the ones above. For this class, we will be dropping them. However, feel free to check them out!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/scotch.csv\")\n",
    "\n",
    "data = data.drop([u'age', u'dist', u'score', u'percent', u'region', u'district', u'islay', u'midland', u'spey', u'east', u'west', u'north ', u'lowland', u'campbell', u'islands'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We've discussed earlier that there are many similarity measures.  Similarity is often cast as \"closeness\" in some space, computed by a distance measure.  Often in data science, the terms similarity and distance are used interchangeably (a little strangely to the uninitiated). \n",
    "\n",
    "We'll use the library scipy.spatial.distance available [here](http://docs.scipy.org/doc/scipy/reference/spatial.distance.html)\n",
    "\n",
    "This library has functions to compute the distance between two **numeric** vectors. In particular, **pdist(X[, metric, p, w, V, VI])**\tcomputes pairwise distances between the observations in n-dimensional space.\n",
    "\n",
    "The _metric_ parameter refers to the **distance function** used to compute the distance between the instances (observations) and can be:\n",
    "* ‘braycurtis’, \n",
    "* ‘canberra’, \n",
    "* ‘chebyshev’, \n",
    "* ‘cityblock’, \n",
    "* ‘correlation’, \n",
    "* ‘cosine’, \n",
    "* ‘dice’, \n",
    "* ‘euclidean’, \n",
    "* ‘hamming’, \n",
    "* ‘jaccard’, \n",
    "* ‘kulsinski’, \n",
    "* ‘mahalanobis’, \n",
    "* ‘matching’, \n",
    "* ‘minkowski’, \n",
    "* ‘rogerstanimoto’, \n",
    "* ‘russellrao’, \n",
    "* ‘seuclidean’, \n",
    "* ‘sokalmichener’, \n",
    "* ‘sokalsneath’, \n",
    "* ‘sqeuclidean’, \n",
    "* ‘yule’\n",
    "\n",
    "Before diving in deep, let's see how the method actually works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean Distance\n",
    "sample_data = [ [1,2,3], [4,5,6], [7,8,9] ]\n",
    "distance.pdist(sample_data, 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, having all of that in the same row isn't particularly helpful. What are we really looking at here?\n",
    "Let's better format our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean Distance\n",
    "sample_data = [ [1,2,3], [4,5,6], [7,8,9] ]\n",
    "distance.squareform(distance.pdist(sample_data, 'euclidean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that's better. Let's do the same for the other distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about Jaccard distance\n",
    "sample_data = [ [0,0,1,1,1], [0,0,1,0,1], [1,0,0,0,1] ]\n",
    "distance.squareform(distance.pdist(sample_data, 'jaccard'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about Hamming distance\n",
    "sample_data = [ [0,0,1,1,1], [0,0,1,0,1], [1,0,0,0,1] ]\n",
    "distance.squareform(distance.pdist(sample_data, 'hamming'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alright, now that we got the hang of it, let's make our life easier as follows.  Here is a function that will compute a number of distances of interest that we will provide ( $distance_measures$ parameter )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def whiskey_distance(whiskey_name, distance_measures, k):\n",
    "    \n",
    "    \"\"\"\n",
    "    whiskey_name: The name of the whiskey to search for\n",
    "    distance_measures: A list containing the different distances that we want to compute\n",
    "    k: The number of similar objects that we want to retrieve\n",
    "    \"\"\"\n",
    "    \n",
    "    # We want a data frame to store the output\n",
    "    # distance_measures is a list of the distance measures you want to compute (see below)\n",
    "    # n is how many \"most similar\" to report\n",
    "    distances = pd.DataFrame()\n",
    "    \n",
    "    # Find the location of the whiskey we are looking for\n",
    "    whiskey_location = np.where(data.index == whiskey_name)[0][0]\n",
    "\n",
    "    # Go through all distance measures we care about\n",
    "    for distance_measure in distance_measures:\n",
    "        # Find all pairwise distances\n",
    "        current_distances = distance.squareform(distance.pdist(data, distance_measure))\n",
    "        # Get the closest n elements for the whiskey we care about\n",
    "        most_similar = np.argsort(current_distances[:, whiskey_location])[0:k]\n",
    "        # Append results (a new column to the dataframe with the name of the measure)\n",
    "        distances[distance_measure] = list(zip(data.index[most_similar], current_distances[most_similar, whiskey_location]))\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's test the distance of one of the whiskeys that are in there\n",
    "whiskey_distance('Bunnahabhain', ['euclidean'], 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about more distances? What is the output then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try more distances for the same whiskey\n",
    "whiskey_distance('Bunnahabhain', ['euclidean', 'cityblock', 'cosine', 'jaccard'], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show me your friends... and I'll make a prediction\n",
    "\n",
    "Is that the only thing we can do with similarity? No, not really.  We can also use similarity between objects (i.e., instances) to make _predictions_. To do so, we need to have labels on the _similar_ instances. How do you think we would do that?\n",
    "\n",
    "See the image below to guide your thinking. We want to classify the green instance. What should its class be?\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/850px-KnnClassification.svg.png\" width=\"30%\" />\n",
    "\n",
    "[Source: Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#/media/File:KnnClassification.svg)\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above approach is known as a **$k$-Nearest Neighbor** (**kNN**) classifier, because it finds the $k$ _most similar_ neighbors of the _instance_ that we want to predict.\n",
    "\n",
    "In the example of the Figure above, we used the Euclidean distance between two items (because... circles!). We collected the $k$ most similar instances in our datasets and combined their target variables to make our prediction.\n",
    "\n",
    "**Question:** How did we combine the target variables of the neighbors?\n",
    "\n",
    "**Question:** How would we combine the target variable for a _regression_ problem?\n",
    "\n",
    "**Question:** **kNN** is a _lazy_ classifier. Can you think why we call it that?\n",
    "\n",
    "**Question:** What happens if we set $k$ to a _very large_ value, e.g., the total number of instances in our dataset?\n",
    "\n",
    "Let's work with our mailing campaign dataset which has a target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mail_data = pd.read_csv(\"data/mailing.csv\")\n",
    "mail_data = pd.get_dummies(mail_data)\n",
    "mail_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"data\" dataframe contains everything together.\n",
    "# Get the features separately from the class.\n",
    "X = mail_data.drop(['class'], axis=1)\n",
    "Y = mail_data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep 75% of the data for training and 25% for testing. We have used this method before.\n",
    "X_mailing_train, X_mailing_test, Y_mailing_train, Y_mailing_test = train_test_split(X, Y, train_size=.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Great! We've read the data in, let's now learn a **kNN** classifier on our data.  The main parameter to consider here is the $k$ value, i.e., the number of neighbors that we want to base our decision making on.  For a full description of the parameters that the **kNN** classifier takes is available [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n",
    "\n",
    "For example, check what happens when the **weights** parameter isn't specified, versus the parameter being set to 'distance', i.e., **weights='distance'**.\n",
    "\n",
    "Also, check out the parameter **p** which controls the power of the Minkowski distance.\n",
    "\n",
    "The **metric** parameter controls which distance measure to consider between instances. More details about the values that the **metric** parameter can take can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a k-Nearest Neighbor classifier\n",
    "model_mailing = KNeighborsClassifier(10, weights='distance')   # Also try it with weights='distance'\n",
    "model_mailing.fit(X_mailing_train, Y_mailing_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the trained model on the testing dataset and get the predicted classes\n",
    "predictions = model_mailing.predict(X_mailing_test)\n",
    "\n",
    "# Let's generate a confusion matrix\n",
    "conf_mtx = metrics.confusion_matrix(Y_mailing_test, predictions, labels=[1, 0])\n",
    "\n",
    "# Let's turn the confusion matrix into a DataFrame, to make it more presentable\n",
    "conf_mtx_df = pd.DataFrame(conf_mtx.T, columns=['(True) p', '(True) n'], index=['[Predicted] Y', '[Predicted] N'])\n",
    "conf_mtx_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it out with some different $k$-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k in [1, 10, 50, 100, 1000, 2000]:\n",
    "    model = KNeighborsClassifier(k, weights='distance')\n",
    "    model.fit(X_mailing_train, Y_mailing_train)\n",
    "    print (\"Accuracy with k = %d is %.3f\" % (k, metrics.accuracy_score(Y_mailing_test, model.predict(X_mailing_test))) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question for you\n",
    "\n",
    "- Advantages of kNN ?\n",
    "- Disadvantages of kNN ?\n",
    "- Also check the previous questions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

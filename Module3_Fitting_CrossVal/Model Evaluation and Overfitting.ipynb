{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining for Business Analytics\n",
    "\n",
    "## Model evaluation and Overfitting\n",
    "\n",
    "Spring 2019 - Prof. George Valkanas\n",
    "\n",
    "Material based on content courtesy of Prof. Foster Provost\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "from dstools import data_tools\n",
    "\n",
    "\n",
    "# for plotting\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "import pydotplus\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = 14, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Well, we've already seen 3 classification techniques so far. Which is the best one?\n",
    "\n",
    "\n",
    "Well, we need a way to evaluate it and measure its performance. How do we do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we run our classification model on our small dataset and we get the following information:\n",
    "\n",
    "<table>\n",
    "    <tr style=\"font-size: large; font-weight: bold; background-color: light-gray\">\n",
    "        <td style=\"border: 2px solid black\"> ID </td> \n",
    "        <td style=\"border: 2px solid black\"> True Label </td> \n",
    "        <td style=\"border: 2px solid black\"> Predicted Label </td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: lightgreen\">\n",
    "        <td style=\"border: 2px solid black\"> 1 </td> <td> 1 </td> <td style=\"border: 2px solid black\"> 1 </td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: lightgreen; border: 2px solid black\">\n",
    "        <td style=\"border: 2px solid black\"> 2 </td> <td> 1 </td> <td style=\"border: 2px solid black\"> 1 </td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: lightgreen; border: 2px solid black\">\n",
    "        <td style=\"border: 2px solid black\"> 3 </td> <td> 0 </td> <td style=\"border: 2px solid black\"> 0 </td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: tomato; border: 2px solid black\">\n",
    "        <td style=\"border: 2px solid black\"> 4 </td> <td> 0 </td> <td style=\"border: 2px solid black\"> 1 </td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: lightgreen; border: 2px solid black\">\n",
    "        <td style=\"border: 2px solid black\"> 5 </td> <td> 0 </td> <td style=\"border: 2px solid black\"> 0 </td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: lightgreen; border: 2px solid black\">\n",
    "        <td style=\"border: 2px solid black\"> 6 </td> <td> 1 </td> <td style=\"border: 2px solid black\"> 1 </td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: tomato; border: 2px solid black\">\n",
    "        <td style=\"border: 2px solid black\"> 7 </td> <td> 1 </td> <td style=\"border: 2px solid black\"> 0 </td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: tomato; border: 2px solid black\">\n",
    "        <td style=\"border: 2px solid black\"> 8 </td> <td> 1 </td> <td style=\"border: 2px solid black\"> 0 </td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: lightgreen; border: 2px solid black\">\n",
    "        <td style=\"border: 2px solid black\"> 9 </td> <td> 0 </td> <td style=\"border: 2px solid black\"> 0 </td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: tomato; border: 2px solid black\">\n",
    "        <td style=\"border: 2px solid black\"> 10 </td> <td> 0 </td> <td style=\"border: 2px solid black\"> 1 </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br/>\n",
    "\n",
    "\n",
    "**Question:** How good would you say that the model is? Can you come up with a measure to evaluate it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model Accuracy\n",
    "\n",
    "\n",
    "OK, so we've established a (simple) measure to evaluate our model. We can use that to compare different data mining techniques / models.\n",
    "\n",
    "Let's go back to our example, that we've all come to love. :-)  We'll learn our classifiers and then apply them to measure their accuracy.\n",
    "\n",
    "We'll save ourselves some time and rely on code that we've written in the past.\n",
    "\n",
    "But is that enough?  Let's discuss this with a simple example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the randomness\n",
    "np.random.seed(36)\n",
    "\n",
    "# Number of users, i.e. number of instances in our dataset\n",
    "n_users = 500\n",
    "\n",
    "# Features that we know about each user. The attributes below are for illustration purposes only!\n",
    "variable_names = [\"name\", \"age\", \"years_customer\"]\n",
    "variables_keep = [\"years_customer\", \"age\"]\n",
    "target_name = \"response\"\n",
    "\n",
    "# Generate data with the \"datasets\" function from SKLEARN (package)\n",
    "# This function returns two variables: predictors and target\n",
    "predictors, target = datasets.make_classification(n_features=3, n_redundant=0, \n",
    "                                                  n_informative=2, n_clusters_per_class=2,\n",
    "                                                  n_samples=n_users)\n",
    "\n",
    "# We will write this data in a dataframe (pandas package)\n",
    "data = pd.DataFrame(predictors, columns=variable_names)\n",
    "\n",
    "# We want to take each column of the dataframe to change the values \n",
    "data['age'] = data['age'] * 10 + 50\n",
    "data['years_customer'] = (data['years_customer'] + 6)/2\n",
    "data[target_name] = target\n",
    "\n",
    "# Our variables (features) will be stored in one variable called X\n",
    "X = data[variables_keep]\n",
    "\n",
    "# Our target will be stored in one variable called Y\n",
    "Y = data[target_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_model_accuracy( model_name, model, data_feats, true_values ):\n",
    "    model_acc = metrics.accuracy_score(model.predict(data_feats), Y)\n",
    "    print ( \"[%s] Accuracy = %.3f\" % (model_name, model_acc) )\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Learn a decision tree, just like the one we've learned in our previous lecture\n",
    "tree_clf = DecisionTreeClassifier(max_depth=10,criterion=\"entropy\")\n",
    "tree_clf.fit(X, Y)\n",
    "print_model_accuracy( \"Decision Tree\", tree_clf, X, Y )\n",
    "\n",
    "# Learn a Logistic Regression model\n",
    "log_reg = LogisticRegression(solver='lbfgs')\n",
    "log_reg.fit(X, Y)\n",
    "print_model_accuracy( \"Logistic Regression\", log_reg, X, Y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about a Decision Trees that aren't that deep ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn a decision tree, just like the one we've learned in our previous lecture\n",
    "tree_clf = DecisionTreeClassifier(max_depth=7,criterion=\"entropy\")\n",
    "tree_clf.fit(X, Y)\n",
    "print_model_accuracy( \"Decision Tree\", tree_clf, X, Y )\n",
    "\n",
    "\n",
    "#And a shorter one?\n",
    "tree_clf = DecisionTreeClassifier(max_depth=5,criterion=\"entropy\")\n",
    "tree_clf.fit(X, Y)\n",
    "print_model_accuracy( \"Decision Tree\", tree_clf, X, Y )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting your data\n",
    "\n",
    "\n",
    "OK. We are onto something here, it seems. The shorter the tree, the worse accuracy that we learn. Why is that though?\n",
    "\n",
    "[Note]: It's a good idea to try and generate the decision surfaces of the above trees (on your own). This will give you a better - more visual - understanding of what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is happening is called `Overfitting`\n",
    "\n",
    "Here's a <a href=\"https://twitter.com/plinz/status/674683831048384514\"> cute explanation</a> of the problem.\n",
    "<br/>\n",
    "\n",
    "\n",
    "**[Refresher on terminology]** Classification VS Regression\n",
    "<br/>\n",
    "\n",
    "Let's better understand what is happening through an example. Instead of a classification problem, let's work a simple numerical dataset where we want to do regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 50\n",
    "\n",
    "# Set randomness so that we all get the same answer\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# This is what the true function is\n",
    "def true_function(X):\n",
    "    return np.sin(1.5 * np.pi * X)\n",
    "\n",
    "\n",
    "def plot_example(X, Y, functions):\n",
    "    # Get some X's to plot the functions\n",
    "    X_test = pd.DataFrame(np.linspace(0, 1, 100), columns=['X'])\n",
    "\n",
    "    # Plot the different functions, using the provided information\n",
    "    for key, fdata in functions.items():\n",
    "        plt.plot(X_test, fdata[\"func\"](X_test), label=key, c=fdata['color'])\n",
    "    plt.scatter(X, Y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    return\n",
    "\n",
    "\n",
    "# Add X in the range of [0, 1]\n",
    "xvals = pd.DataFrame(np.sort(np.random.rand(num_samples)), columns=['x1'])\n",
    "# Add some random noise to the observations\n",
    "yvals = true_function(xvals.x1) + np.random.randn(num_samples) * 0.5\n",
    "# Plot stuff\n",
    "functions = {\"True function\": {\"func\" : true_function, \"color\" : 'g' }}\n",
    "plot_example(xvals, yvals, functions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the `True function` for now. Besides, we wouldn't really know it, and we would only have access to the observations.\n",
    "\n",
    "Let's start simple and build a basic linear model, as we do below. This is just a Linear Regression model.\n",
    "Because we are dealing with purely numerical values (no categorical data), we'll use the `mean squared error` as a way to measure the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Fit linear model\n",
    "model = LinearRegression()\n",
    "model.fit(xvals, yvals)\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(xvals, model.predict(xvals))\n",
    "# Plot results\n",
    "functions[\"Model\"] = {'func' : model.predict, 'color' : 'r'}\n",
    "plot_example(xvals, yvals, functions)\n",
    "plt.title(\"Linear Model - MSE: %.2f\" % mse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that doesn't look great, but that's to be expected, because we knew that the `True function` is not a linear one.\n",
    "\n",
    "Let's try to fit a higher-order polynomial function instead, something like the following:\n",
    "\n",
    "$$ y = w_0 + w_1 * x + w_2 * x^2 + ... $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def fit_polynomial(xvals, yvals, degree):\n",
    "    # create different powers of X\n",
    "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features), (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(xvals, yvals)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_poly(xvals, yvals, degree, color):\n",
    "    # Fit polynomial model\n",
    "    model = fit_polynomial(xvals, yvals, degree)\n",
    "    # Evaluate model. This is still regression, so we're doing again mean square error\n",
    "    mse = mean_squared_error(yvals, model.predict(xvals))\n",
    "    # Plot results\n",
    "    functions[\"Model\"] = {'func' : model.predict, 'color' : color } \n",
    "    plt.title(\"Degree %d - MSE: %.2f\" % (degree, mse))\n",
    "    plot_example(xvals, yvals, functions)\n",
    "    return\n",
    "    \n",
    "plot_poly(xvals, yvals, degree=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!! That's better, right? It seems that this higher-order polynomial is doing the trick. Let's try again with some higher orders and plot everything side-by-side for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 14, 10\n",
    "\n",
    "# Let's learn multiple models, with different polynomial orders, side by side so that we can compare them\n",
    "degrees = [1, 2, 5, 10, 20, 50]\n",
    "for i, d in enumerate(degrees):\n",
    "    ax = plt.subplot( 2, 3, i + 1)\n",
    "    ax.figsize=[6,8]\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "    plot_poly(xvals, yvals, d, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What just happened!?\n",
    "\n",
    "The model that we are learning tries to fit the observations **in the best way possible**, but captures a <u>lot of noise</u> along the way, instead of the general \"trend\" of the true function.\n",
    "\n",
    "Let's go back to our binary classification example. The problem in that setting looks like so:\n",
    "\n",
    "<img width=\"30%\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/2000px-Overfitting.svg.png\" />\n",
    "\n",
    "[Source: Wikipedia]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create another dataset to look at logistic regression again. We still have two features, but the dataset isn't as nicely separated like we've seen in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some data: The create_data() function returns 4 variables:\n",
    "target_name, variable_names, data, y_nonlin = data_tools.create_data()\n",
    "\n",
    "# Grab the predictors (rows and columns)\n",
    "x_nonlin = data_tools.X()\n",
    "\n",
    "# Model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(x_nonlin, y_nonlin)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[10,7])\n",
    "data_tools.Decision_Surface(x_nonlin, y_nonlin, model, probabilities=False)\n",
    "plt.title(\"Linear model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy of that model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.score(x_nonlin,y_nonlin)\n",
    "print('Training Accuracy: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "max_order = 3  # Maximum of 3 was created in the script:  data_tools.py\n",
    "\n",
    "nrows = np.floor(np.sqrt(max_order))\n",
    "ncol = 2 if max_order == 4 else 3\n",
    "plt.figure(figsize=[15,7*nrows])\n",
    "\n",
    "for order in range(1, max_order+1):\n",
    "    # Get a dataset X_complex with non linear variables\n",
    "    X_complex = data_tools.X(order)\n",
    "    \n",
    "    # Model used to predict\n",
    "    model = LogisticRegression(solver='lbfgs') # penalty='l2'\n",
    "    model.fit(X_complex, y_nonlin)\n",
    "\n",
    "    # Plot and calculate accuracy\n",
    "    plt.subplot(nrows, ncol, order)\n",
    "    data_tools.Decision_Surface(X_complex, y_nonlin, model, probabilities=False)\n",
    "    acc_value = metrics.accuracy_score(model.predict(X_complex), y_nonlin) \n",
    "    plt.title(\"Linear model \" + str(order) + \"-order (accuracy: \"+ str(round(acc_value,3))+\")\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show probabilities?\n",
    "show_probabilities = False\n",
    "\n",
    "# Array of penalty values\n",
    "penalties = [1,2,3,5]\n",
    "penalty_type = \"l1\"\n",
    "\n",
    "# set tne number of rows and columns for the plots\n",
    "ncol = 2\n",
    "nrows = np.ceil(len(penalties)/ncol)\n",
    "plt.figure(figsize=[15,7*nrows])\n",
    "\n",
    "idx = 1 # index value (used for arranging the plots)\n",
    "for inv_penalty in penalties:\n",
    "    # Get a dataset X_complex with non linear variables\n",
    "    X_complex = data_tools.X(max_order)\n",
    "    \n",
    "    # Model used to predict\n",
    "    model = LogisticRegression(penalty=penalty_type,C=10**(-inv_penalty))\n",
    "    model.fit(X_complex, y_nonlin)\n",
    "    \n",
    "    # Plot and calculate accuracy\n",
    "\n",
    "    plt.subplot(nrows, ncol,idx)\n",
    "    idx = idx +1\n",
    "    data_tools.Decision_Surface(X_complex, y_nonlin, model, probabilities=show_probabilities)\n",
    "    acc_value = metrics.accuracy_score(model.predict(X_complex), y_nonlin) \n",
    "    plt.title(\"Linear model \" + str(order) + \"-order, \"+penalty_type.upper()+\" penalty: 10^\"+str(inv_penalty)+\"  (accuracy: \"+ str(round(acc_value,3))+\")\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model is better in this case?? Look at the **accuracy** of each one.   Accuracy is simply the count of correct decisions divided by the total number of decisions.\n",
    "\n",
    "[From sklearn documentation on sklearn.metrics.accuracy_score: \"In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\"  [More about the accuracy measure..](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)]\n",
    "\n",
    "Of course, we can also look at the **probabilities** on these non-linear surfaces. Try it out above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization\n",
    "\n",
    "Our evaluation above actually was not what we really want.\n",
    "\n",
    "What we want are models that **generalize** to data that were not used to build them! In other words, we want this model to be able to predict the target for new data instances! Do we know how well our models generalize? Why is this important?\n",
    "\n",
    "<img src=\"images/generalization.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "Let's apply this concept to our data. Now, before we fit out models, we set aside some data to be used later for testing ('holdout data').  This allows us to assess whether the model simply fit the training dataset well, or whether it truly fit some regularities in the domain. \n",
    "\n",
    "Let's use sklearn to set aside some randomly selected holdout data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set randomness so that we all get the same answer\n",
    "np.random.seed(842)\n",
    "\n",
    "# Split the data into train and test pieces for both X and Y\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_nonlin, y_nonlin, train_size=0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split our data, let's revisit the tree-structured classifier. Let's check how well a model does when it is fit on a training set and then used to predict on both the training set as well as our holdout set. Remember, the model has never seen this holdout \"test\" set before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print ( \"Accuracy on training = %.4f\" % metrics.accuracy_score(model.predict(X_train), Y_train) )\n",
    "print ( \"Accuracy on test = %.4f\" % metrics.accuracy_score(model.predict(X_test), Y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results on the test set were worse. Why is this? Can it ever do beter?\n",
    "\n",
    "What happens as our tree gets more and more complicated?  (Deeper and deeper.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "maxdepth = 20\n",
    "depths = range(1, maxdepth+1)\n",
    "\n",
    "plt.figure(figsize=[10,7])\n",
    "\n",
    "for md in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=md)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    accuracies_train.append(metrics.accuracy_score(model.predict(X_train), Y_train))\n",
    "    accuracies_test.append(metrics.accuracy_score(model.predict(X_test), Y_test))\n",
    "\n",
    "plt.plot(depths, accuracies_train, label=\"Train\")\n",
    "plt.plot(depths, accuracies_test, label=\"Test\")\n",
    "plt.title(\"Performance on train and test data\")\n",
    "plt.xlabel(\"Max depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([min(accuracies_test), 1.0])\n",
    "plt.xlim([1,maxdepth])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Above, we made a single train/test split. We set aside 20% of our data and *never* used it for training. We also never used the 80% of the data set aside for training to test generalizability.  Although this is far better than testing on the training data, which does not measure generalization performance at all, there are two potential problems with the simple holdout approach.\n",
    "\n",
    "1) Perhaps the random split was particularly bad (or good).  Do we have any confidence in our accuracy estimate?\n",
    "\n",
    "2) We are using only 20\\% of the data for testing.  Could we possibly use the data more fully for testing?\n",
    "\n",
    "Instead of only making the split once, let's use \"cross-validation\" -- every record will contribute to testing as well as to training.\n",
    "\n",
    "\n",
    "<img src=\"images/cross.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = DecisionTreeClassifier(max_depth=1)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print ( \"Cross validation accuracy on training = %.3f\" % np.mean(cross_val_score(model, x_nonlin, y_nonlin)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add this to our plot from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracies_cross_validation = []\n",
    "depths = range(1, 21)\n",
    "\n",
    "plt.figure(figsize=[10,7])\n",
    "\n",
    "for md in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=md)\n",
    "    \n",
    "    accuracies_cross_validation.append(np.mean(cross_val_score(model, x_nonlin, y_nonlin,cv=10)))\n",
    "\n",
    "plt.plot(depths, accuracies_train, label=\"Train\")\n",
    "plt.plot(depths, accuracies_test, label=\"Test\")\n",
    "plt.title(\"Performace on train and test data\")\n",
    "plt.plot(depths, accuracies_cross_validation, label=\"Cross Validation\")\n",
    "plt.xlabel(\"Max depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([min(accuracies_cross_validation) , 1.0])\n",
    "plt.xlim([1,20])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does a deeper try fit the training data better, but not the test or holdout data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
